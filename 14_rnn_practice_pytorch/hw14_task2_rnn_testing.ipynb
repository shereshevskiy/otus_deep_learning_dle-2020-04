{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "hw14_task2_rnn_testing.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgSTDtIr7YnK",
        "colab_type": "text"
      },
      "source": [
        "# <center>Домашнее задание\n",
        "# <center>Создаем Википедию\n",
        "(Задание 1. Используя подход аналогичный torchvision, сделать свой класс датасета - см. файл hw14_task1) \n",
        "## Задание 2. Необязательное д/з:   \n",
        "## Поэкспериментировать с разными архитектурами рекурренток: тип ячеек, слои, нормализация, методы оптимизации)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcfxLKw76iFJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d6fbf57-1cde-4938-b444-c3b3fe23d320"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJZYVPaj68gG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('drive/My Drive/Colab Notebooks/neural_OTUS')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4b_LCL26AUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math \n",
        "\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "from wiki2_utils import WikiText2Dataset\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RrpV0hy8HwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c822b546-ca4a-42fe-a089-5282f036f293"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4ajq6Yl7f9i",
        "colab_type": "text"
      },
      "source": [
        "## <center>2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zh4fqTY6AUd",
        "colab_type": "text"
      },
      "source": [
        "#### параметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbHQoZ_k6AUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "sequence_length = 30\n",
        "grad_clip = 0.1\n",
        "lr = 4.\n",
        "best_val_loss = None\n",
        "log_interval = 100\n",
        "\n",
        "eval_batch_size = 128"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMC1FxKW6AUg",
        "colab_type": "text"
      },
      "source": [
        "#### загрузчики датасетов (с использованием нового класса)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0BEPMAq6AUh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ddb39d98-d28e-4af1-c3b8-dcd6a28564ea"
      },
      "source": [
        "%%time\n",
        "\n",
        "dataset = WikiText2Dataset(batch_size=batch_size, eval_batch_size=eval_batch_size, \n",
        "                           sequence_length=sequence_length, device=device)\n",
        "\n",
        "train_loader = dataset.get_train_loader()\n",
        "val_loader = dataset.get_val_loader()\n",
        "test_loader = dataset.get_test_loader()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.02 s, sys: 782 ms, total: 4.8 s\n",
            "Wall time: 4.82 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfMe7Ja5MMV4",
        "colab_type": "text"
      },
      "source": [
        "## 1) Повторим исходный результат:\n",
        "\n",
        "\n",
        "*   LSTM\n",
        "*   nlayers = 2\n",
        "* dropout = 0.5\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZM1d6n46AUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        elif rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        emb = self.drop(self.encoder(x))  # 30x128x128(ninp - размерность единицы на входе)\n",
        "        output, hidden = self.rnn(emb, hidden)  # 30x128x128(nhid - размерность единицы на выходе (внутр. слоя))\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
        "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
        "        else:\n",
        "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqs3kjRT6AUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(data_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    ntokens = len(dataset.idx2symbol)\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    for i, (data, targets) in enumerate(data_loader):\n",
        "        output, hidden = model(data)\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "        total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / len(data_loader)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79GE2GJH6AUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVpJOar76AUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    ntokens = len(dataset.idx2symbol)\n",
        "    for batch, (data, targets) in enumerate(train_loader):\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_loader) // sequence_length, lr, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG3EwMNY6AU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens = len(dataset.idx2symbol)\n",
        "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3).to(device)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SONA42gB6AU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(n=50, temp=1.):\n",
        "    model.eval()\n",
        "    x = torch.rand(1, 1).mul(ntokens).long().to(device)\n",
        "    hidden = None\n",
        "    out = []\n",
        "    for i in range(n):\n",
        "        output, hidden = model(x, hidden)\n",
        "        s_weights = output.squeeze().data.div(temp).exp()\n",
        "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
        "        x.data.fill_(s_idx)\n",
        "        s = dataset.idx2symbol[s_idx]\n",
        "        out.append(s)\n",
        "    return ''.join(out)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PSFiZ096AU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aycyRqU96AVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9283e21-0746-4a10-9535-c1822d1bc4b9"
      },
      "source": [
        "%%time\n",
        "\n",
        "with torch.no_grad():\n",
        "    print('sample:\\n', generate(50), '\\n')\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    train()\n",
        "    val_loss = evaluate(val_loader)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "        epoch, val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        lr /= 4.0\n",
        "    with torch.no_grad():\n",
        "        print('sample:\\n', generate(50), '\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample:\n",
            " lćხოØŻł<eos>îöŌჯʻảử[ตś0+6რ$ณDṃKL:V礮íųュđμ^﻿²°ấ्機yưยḥhă− \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   100/ 2807 batches | lr 4.00 | loss  3.59 | ppl    36.38\n",
            "| epoch   1 |   200/ 2807 batches | lr 4.00 | loss  3.28 | ppl    26.67\n",
            "| epoch   1 |   300/ 2807 batches | lr 4.00 | loss  3.25 | ppl    25.78\n",
            "| epoch   1 |   400/ 2807 batches | lr 4.00 | loss  3.22 | ppl    25.12\n",
            "| epoch   1 |   500/ 2807 batches | lr 4.00 | loss  3.20 | ppl    24.53\n",
            "| epoch   1 |   600/ 2807 batches | lr 4.00 | loss  3.07 | ppl    21.53\n",
            "| epoch   1 |   700/ 2807 batches | lr 4.00 | loss  2.98 | ppl    19.64\n",
            "| epoch   1 |   800/ 2807 batches | lr 4.00 | loss  2.89 | ppl    18.00\n",
            "| epoch   1 |   900/ 2807 batches | lr 4.00 | loss  2.81 | ppl    16.67\n",
            "| epoch   1 |  1000/ 2807 batches | lr 4.00 | loss  2.73 | ppl    15.37\n",
            "| epoch   1 |  1100/ 2807 batches | lr 4.00 | loss  2.63 | ppl    13.91\n",
            "| epoch   1 |  1200/ 2807 batches | lr 4.00 | loss  2.56 | ppl    12.97\n",
            "| epoch   1 |  1300/ 2807 batches | lr 4.00 | loss  2.51 | ppl    12.28\n",
            "| epoch   1 |  1400/ 2807 batches | lr 4.00 | loss  2.46 | ppl    11.67\n",
            "| epoch   1 |  1500/ 2807 batches | lr 4.00 | loss  2.42 | ppl    11.29\n",
            "| epoch   1 |  1600/ 2807 batches | lr 4.00 | loss  2.39 | ppl    10.92\n",
            "| epoch   1 |  1700/ 2807 batches | lr 4.00 | loss  2.35 | ppl    10.53\n",
            "| epoch   1 |  1800/ 2807 batches | lr 4.00 | loss  2.32 | ppl    10.21\n",
            "| epoch   1 |  1900/ 2807 batches | lr 4.00 | loss  2.30 | ppl     9.99\n",
            "| epoch   1 |  2000/ 2807 batches | lr 4.00 | loss  2.27 | ppl     9.67\n",
            "| epoch   1 |  2100/ 2807 batches | lr 4.00 | loss  2.25 | ppl     9.47\n",
            "| epoch   1 |  2200/ 2807 batches | lr 4.00 | loss  2.22 | ppl     9.24\n",
            "| epoch   1 |  2300/ 2807 batches | lr 4.00 | loss  2.22 | ppl     9.18\n",
            "| epoch   1 |  2400/ 2807 batches | lr 4.00 | loss  2.19 | ppl     8.92\n",
            "| epoch   1 |  2500/ 2807 batches | lr 4.00 | loss  2.18 | ppl     8.80\n",
            "| epoch   1 |  2600/ 2807 batches | lr 4.00 | loss  2.16 | ppl     8.65\n",
            "| epoch   1 |  2700/ 2807 batches | lr 4.00 | loss  2.14 | ppl     8.52\n",
            "| epoch   1 |  2800/ 2807 batches | lr 4.00 | loss  2.11 | ppl     8.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | valid loss  1.94 | valid ppl     6.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  Wortpamel her the pentianly , in devents Waw they \n",
            "\n",
            "| epoch   2 |   100/ 2807 batches | lr 4.00 | loss  2.12 | ppl     8.36\n",
            "| epoch   2 |   200/ 2807 batches | lr 4.00 | loss  2.08 | ppl     8.04\n",
            "| epoch   2 |   300/ 2807 batches | lr 4.00 | loss  2.07 | ppl     7.93\n",
            "| epoch   2 |   400/ 2807 batches | lr 4.00 | loss  2.06 | ppl     7.86\n",
            "| epoch   2 |   500/ 2807 batches | lr 4.00 | loss  2.05 | ppl     7.76\n",
            "| epoch   2 |   600/ 2807 batches | lr 4.00 | loss  2.04 | ppl     7.67\n",
            "| epoch   2 |   700/ 2807 batches | lr 4.00 | loss  2.03 | ppl     7.61\n",
            "| epoch   2 |   800/ 2807 batches | lr 4.00 | loss  2.02 | ppl     7.53\n",
            "| epoch   2 |   900/ 2807 batches | lr 4.00 | loss  2.02 | ppl     7.50\n",
            "| epoch   2 |  1000/ 2807 batches | lr 4.00 | loss  2.01 | ppl     7.44\n",
            "| epoch   2 |  1100/ 2807 batches | lr 4.00 | loss  1.99 | ppl     7.30\n",
            "| epoch   2 |  1200/ 2807 batches | lr 4.00 | loss  1.98 | ppl     7.27\n",
            "| epoch   2 |  1300/ 2807 batches | lr 4.00 | loss  1.97 | ppl     7.19\n",
            "| epoch   2 |  1400/ 2807 batches | lr 4.00 | loss  1.96 | ppl     7.08\n",
            "| epoch   2 |  1500/ 2807 batches | lr 4.00 | loss  1.95 | ppl     7.06\n",
            "| epoch   2 |  1600/ 2807 batches | lr 4.00 | loss  1.95 | ppl     7.04\n",
            "| epoch   2 |  1700/ 2807 batches | lr 4.00 | loss  1.94 | ppl     6.97\n",
            "| epoch   2 |  1800/ 2807 batches | lr 4.00 | loss  1.93 | ppl     6.92\n",
            "| epoch   2 |  1900/ 2807 batches | lr 4.00 | loss  1.94 | ppl     6.94\n",
            "| epoch   2 |  2000/ 2807 batches | lr 4.00 | loss  1.92 | ppl     6.83\n",
            "| epoch   2 |  2100/ 2807 batches | lr 4.00 | loss  1.92 | ppl     6.83\n",
            "| epoch   2 |  2200/ 2807 batches | lr 4.00 | loss  1.91 | ppl     6.78\n",
            "| epoch   2 |  2300/ 2807 batches | lr 4.00 | loss  1.92 | ppl     6.79\n",
            "| epoch   2 |  2400/ 2807 batches | lr 4.00 | loss  1.90 | ppl     6.67\n",
            "| epoch   2 |  2500/ 2807 batches | lr 4.00 | loss  1.89 | ppl     6.64\n",
            "| epoch   2 |  2600/ 2807 batches | lr 4.00 | loss  1.90 | ppl     6.66\n",
            "| epoch   2 |  2700/ 2807 batches | lr 4.00 | loss  1.89 | ppl     6.63\n",
            "| epoch   2 |  2800/ 2807 batches | lr 4.00 | loss  1.87 | ppl     6.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | valid loss  1.67 | valid ppl     5.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  Pireeust to <unk> when Exaly Song = <eos> <eos> Histract  \n",
            "\n",
            "| epoch   3 |   100/ 2807 batches | lr 4.00 | loss  1.89 | ppl     6.63\n",
            "| epoch   3 |   200/ 2807 batches | lr 4.00 | loss  1.86 | ppl     6.44\n",
            "| epoch   3 |   300/ 2807 batches | lr 4.00 | loss  1.86 | ppl     6.43\n",
            "| epoch   3 |   400/ 2807 batches | lr 4.00 | loss  1.86 | ppl     6.42\n",
            "| epoch   3 |   500/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.39\n",
            "| epoch   3 |   600/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.35\n",
            "| epoch   3 |   700/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.35\n",
            "| epoch   3 |   800/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.30\n",
            "| epoch   3 |   900/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.30\n",
            "| epoch   3 |  1000/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.30\n",
            "| epoch   3 |  1100/ 2807 batches | lr 4.00 | loss  1.83 | ppl     6.22\n",
            "| epoch   3 |  1200/ 2807 batches | lr 4.00 | loss  1.83 | ppl     6.25\n",
            "| epoch   3 |  1300/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.19\n",
            "| epoch   3 |  1400/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.12\n",
            "| epoch   3 |  1500/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.14\n",
            "| epoch   3 |  1600/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.16\n",
            "| epoch   3 |  1700/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.09\n",
            "| epoch   3 |  1800/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.09\n",
            "| epoch   3 |  1900/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.15\n",
            "| epoch   3 |  2000/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
            "| epoch   3 |  2100/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.10\n",
            "| epoch   3 |  2200/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.06\n",
            "| epoch   3 |  2300/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.09\n",
            "| epoch   3 |  2400/ 2807 batches | lr 4.00 | loss  1.79 | ppl     5.99\n",
            "| epoch   3 |  2500/ 2807 batches | lr 4.00 | loss  1.79 | ppl     5.98\n",
            "| epoch   3 |  2600/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.03\n",
            "| epoch   3 |  2700/ 2807 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
            "| epoch   3 |  2800/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | valid loss  1.57 | valid ppl     4.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " ầ Gerson <unk> mearing thear town and to base of L \n",
            "\n",
            "| epoch   4 |   100/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.05\n",
            "| epoch   4 |   200/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.90\n",
            "| epoch   4 |   300/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.90\n",
            "| epoch   4 |   400/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.91\n",
            "| epoch   4 |   500/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
            "| epoch   4 |   600/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
            "| epoch   4 |   700/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
            "| epoch   4 |   800/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.84\n",
            "| epoch   4 |   900/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.85\n",
            "| epoch   4 |  1000/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
            "| epoch   4 |  1100/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
            "| epoch   4 |  1200/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.84\n",
            "| epoch   4 |  1300/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.80\n",
            "| epoch   4 |  1400/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
            "| epoch   4 |  1500/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
            "| epoch   4 |  1600/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.77\n",
            "| epoch   4 |  1700/ 2807 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
            "| epoch   4 |  1800/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
            "| epoch   4 |  1900/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.79\n",
            "| epoch   4 |  2000/ 2807 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
            "| epoch   4 |  2100/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
            "| epoch   4 |  2200/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
            "| epoch   4 |  2300/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
            "| epoch   4 |  2400/ 2807 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
            "| epoch   4 |  2500/ 2807 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
            "| epoch   4 |  2600/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
            "| epoch   4 |  2700/ 2807 batches | lr 4.00 | loss  1.74 | ppl     5.71\n",
            "| epoch   4 |  2800/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | valid loss  1.52 | valid ppl     4.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " oce and sixhes of latar , eha @-@ @-@ meterialle c \n",
            "\n",
            "| epoch   5 |   100/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
            "| epoch   5 |   200/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.61\n",
            "| epoch   5 |   300/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
            "| epoch   5 |   400/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
            "| epoch   5 |   500/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
            "| epoch   5 |   600/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.61\n",
            "| epoch   5 |   700/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
            "| epoch   5 |   800/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
            "| epoch   5 |   900/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
            "| epoch   5 |  1000/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
            "| epoch   5 |  1100/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
            "| epoch   5 |  1200/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.61\n",
            "| epoch   5 |  1300/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
            "| epoch   5 |  1400/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.50\n",
            "| epoch   5 |  1500/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
            "| epoch   5 |  1600/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
            "| epoch   5 |  1700/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
            "| epoch   5 |  1800/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
            "| epoch   5 |  1900/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
            "| epoch   5 |  2000/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
            "| epoch   5 |  2100/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
            "| epoch   5 |  2200/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
            "| epoch   5 |  2300/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
            "| epoch   5 |  2400/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.50\n",
            "| epoch   5 |  2500/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.49\n",
            "| epoch   5 |  2600/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
            "| epoch   5 |  2700/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
            "| epoch   5 |  2800/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | valid loss  1.49 | valid ppl     4.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " urs strancester was epanded of shins proxecture a  \n",
            "\n",
            "CPU times: user 2min 50s, sys: 1min, total: 3min 50s\n",
            "Wall time: 3min 51s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8PChYZp6AVF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3785cff9-985e-4f83-f021-6fccf4e964a8"
      },
      "source": [
        "%%time\n",
        "\n",
        "t1 = generate(10000, 1.)\n",
        "t15 = generate(10000, 1.5)\n",
        "t075 = generate(10000, 0.75)\n",
        "with open('./generated075.txt', 'w', encoding=\"utf-8\") as outf:\n",
        "    outf.write(t075)\n",
        "with open('./generated1.txt', 'w', encoding=\"utf-8\") as outf:\n",
        "    outf.write(t1)\n",
        "with open('./generated15.txt', 'w', encoding=\"utf-8\") as outf:\n",
        "    outf.write(t15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 21 s, sys: 619 ms, total: 21.6 s\n",
            "Wall time: 21.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaGGrLTAN4JR",
        "colab_type": "text"
      },
      "source": [
        "## Будем делать по 1 модификации и сравнивать с исходным бейзлайном: \n",
        "### | end of epoch   5 | valid loss  1.49 | valid ppl     4.42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNxabTCGNFAi",
        "colab_type": "text"
      },
      "source": [
        "## 2) GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sD4fMyl6AVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens = len(dataset.idx2symbol)\n",
        "model = RNNModel('GRU', ntokens, 128, 128, 2, 0.3).to(device)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xMZUYkHNoW6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa1dd69c-11db-4d95-922f-40499bbe4845"
      },
      "source": [
        "%%time\n",
        "\n",
        "with torch.no_grad():\n",
        "    print('sample:\\n', generate(50), '\\n')\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    train()\n",
        "    val_loss = evaluate(val_loader)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "        epoch, val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        lr /= 4.0\n",
        "    with torch.no_grad():\n",
        "        print('sample:\\n', generate(50), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample:\n",
            " ม<pad>์プMგÆ:еA'ძHаy−!्|NQC8b–ê&3pRīúńñв☉<unk>〉ư場C჻¥аยX्₹X$ \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   100/ 2807 batches | lr 4.00 | loss  3.58 | ppl    35.93\n",
            "| epoch   1 |   200/ 2807 batches | lr 4.00 | loss  3.26 | ppl    26.07\n",
            "| epoch   1 |   300/ 2807 batches | lr 4.00 | loss  3.10 | ppl    22.28\n",
            "| epoch   1 |   400/ 2807 batches | lr 4.00 | loss  2.92 | ppl    18.63\n",
            "| epoch   1 |   500/ 2807 batches | lr 4.00 | loss  2.74 | ppl    15.44\n",
            "| epoch   1 |   600/ 2807 batches | lr 4.00 | loss  2.62 | ppl    13.70\n",
            "| epoch   1 |   700/ 2807 batches | lr 4.00 | loss  2.52 | ppl    12.45\n",
            "| epoch   1 |   800/ 2807 batches | lr 4.00 | loss  2.45 | ppl    11.57\n",
            "| epoch   1 |   900/ 2807 batches | lr 4.00 | loss  2.40 | ppl    11.04\n",
            "| epoch   1 |  1000/ 2807 batches | lr 4.00 | loss  2.36 | ppl    10.61\n",
            "| epoch   1 |  1100/ 2807 batches | lr 4.00 | loss  2.32 | ppl    10.15\n",
            "| epoch   1 |  1200/ 2807 batches | lr 4.00 | loss  2.29 | ppl     9.87\n",
            "| epoch   1 |  1300/ 2807 batches | lr 4.00 | loss  2.26 | ppl     9.62\n",
            "| epoch   1 |  1400/ 2807 batches | lr 4.00 | loss  2.23 | ppl     9.34\n",
            "| epoch   1 |  1500/ 2807 batches | lr 4.00 | loss  2.22 | ppl     9.19\n",
            "| epoch   1 |  1600/ 2807 batches | lr 4.00 | loss  2.20 | ppl     9.03\n",
            "| epoch   1 |  1700/ 2807 batches | lr 4.00 | loss  2.18 | ppl     8.83\n",
            "| epoch   1 |  1800/ 2807 batches | lr 4.00 | loss  2.16 | ppl     8.70\n",
            "| epoch   1 |  1900/ 2807 batches | lr 4.00 | loss  2.15 | ppl     8.59\n",
            "| epoch   1 |  2000/ 2807 batches | lr 4.00 | loss  2.13 | ppl     8.39\n",
            "| epoch   1 |  2100/ 2807 batches | lr 4.00 | loss  2.12 | ppl     8.31\n",
            "| epoch   1 |  2200/ 2807 batches | lr 4.00 | loss  2.10 | ppl     8.18\n",
            "| epoch   1 |  2300/ 2807 batches | lr 4.00 | loss  2.10 | ppl     8.17\n",
            "| epoch   1 |  2400/ 2807 batches | lr 4.00 | loss  2.08 | ppl     7.99\n",
            "| epoch   1 |  2500/ 2807 batches | lr 4.00 | loss  2.07 | ppl     7.93\n",
            "| epoch   1 |  2600/ 2807 batches | lr 4.00 | loss  2.06 | ppl     7.86\n",
            "| epoch   1 |  2700/ 2807 batches | lr 4.00 | loss  2.05 | ppl     7.79\n",
            "| epoch   1 |  2800/ 2807 batches | lr 4.00 | loss  2.03 | ppl     7.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | valid loss  1.83 | valid ppl     6.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  , wrows Noutlore in 1477 , by care forssonoed mil \n",
            "\n",
            "| epoch   2 |   100/ 2807 batches | lr 4.00 | loss  2.04 | ppl     7.73\n",
            "| epoch   2 |   200/ 2807 batches | lr 4.00 | loss  2.01 | ppl     7.47\n",
            "| epoch   2 |   300/ 2807 batches | lr 4.00 | loss  2.01 | ppl     7.43\n",
            "| epoch   2 |   400/ 2807 batches | lr 4.00 | loss  2.00 | ppl     7.39\n",
            "| epoch   2 |   500/ 2807 batches | lr 4.00 | loss  1.99 | ppl     7.32\n",
            "| epoch   2 |   600/ 2807 batches | lr 4.00 | loss  1.98 | ppl     7.27\n",
            "| epoch   2 |   700/ 2807 batches | lr 4.00 | loss  1.98 | ppl     7.25\n",
            "| epoch   2 |   800/ 2807 batches | lr 4.00 | loss  1.97 | ppl     7.19\n",
            "| epoch   2 |   900/ 2807 batches | lr 4.00 | loss  1.97 | ppl     7.16\n",
            "| epoch   2 |  1000/ 2807 batches | lr 4.00 | loss  1.97 | ppl     7.15\n",
            "| epoch   2 |  1100/ 2807 batches | lr 4.00 | loss  1.95 | ppl     7.06\n",
            "| epoch   2 |  1200/ 2807 batches | lr 4.00 | loss  1.95 | ppl     7.04\n",
            "| epoch   2 |  1300/ 2807 batches | lr 4.00 | loss  1.94 | ppl     6.99\n",
            "| epoch   2 |  1400/ 2807 batches | lr 4.00 | loss  1.93 | ppl     6.89\n",
            "| epoch   2 |  1500/ 2807 batches | lr 4.00 | loss  1.93 | ppl     6.92\n",
            "| epoch   2 |  1600/ 2807 batches | lr 4.00 | loss  1.93 | ppl     6.89\n",
            "| epoch   2 |  1700/ 2807 batches | lr 4.00 | loss  1.92 | ppl     6.84\n",
            "| epoch   2 |  1800/ 2807 batches | lr 4.00 | loss  1.92 | ppl     6.81\n",
            "| epoch   2 |  1900/ 2807 batches | lr 4.00 | loss  1.92 | ppl     6.85\n",
            "| epoch   2 |  2000/ 2807 batches | lr 4.00 | loss  1.91 | ppl     6.76\n",
            "| epoch   2 |  2100/ 2807 batches | lr 4.00 | loss  1.91 | ppl     6.78\n",
            "| epoch   2 |  2200/ 2807 batches | lr 4.00 | loss  1.91 | ppl     6.74\n",
            "| epoch   2 |  2300/ 2807 batches | lr 4.00 | loss  1.91 | ppl     6.76\n",
            "| epoch   2 |  2400/ 2807 batches | lr 4.00 | loss  1.89 | ppl     6.65\n",
            "| epoch   2 |  2500/ 2807 batches | lr 4.00 | loss  1.90 | ppl     6.66\n",
            "| epoch   2 |  2600/ 2807 batches | lr 4.00 | loss  1.90 | ppl     6.68\n",
            "| epoch   2 |  2700/ 2807 batches | lr 4.00 | loss  1.90 | ppl     6.66\n",
            "| epoch   2 |  2800/ 2807 batches | lr 4.00 | loss  1.88 | ppl     6.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | valid loss  1.66 | valid ppl     5.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  . Tunpolact in Moirland \" \" WMet ( <unk> , and co \n",
            "\n",
            "| epoch   3 |   100/ 2807 batches | lr 4.00 | loss  1.90 | ppl     6.70\n",
            "| epoch   3 |   200/ 2807 batches | lr 4.00 | loss  1.87 | ppl     6.51\n",
            "| epoch   3 |   300/ 2807 batches | lr 4.00 | loss  1.87 | ppl     6.51\n",
            "| epoch   3 |   400/ 2807 batches | lr 4.00 | loss  1.87 | ppl     6.51\n",
            "| epoch   3 |   500/ 2807 batches | lr 4.00 | loss  1.87 | ppl     6.49\n",
            "| epoch   3 |   600/ 2807 batches | lr 4.00 | loss  1.86 | ppl     6.45\n",
            "| epoch   3 |   700/ 2807 batches | lr 4.00 | loss  1.87 | ppl     6.47\n",
            "| epoch   3 |   800/ 2807 batches | lr 4.00 | loss  1.86 | ppl     6.43\n",
            "| epoch   3 |   900/ 2807 batches | lr 4.00 | loss  1.86 | ppl     6.44\n",
            "| epoch   3 |  1000/ 2807 batches | lr 4.00 | loss  1.86 | ppl     6.43\n",
            "| epoch   3 |  1100/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.38\n",
            "| epoch   3 |  1200/ 2807 batches | lr 4.00 | loss  1.86 | ppl     6.42\n",
            "| epoch   3 |  1300/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.36\n",
            "| epoch   3 |  1400/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.29\n",
            "| epoch   3 |  1500/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.33\n",
            "| epoch   3 |  1600/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.33\n",
            "| epoch   3 |  1700/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.28\n",
            "| epoch   3 |  1800/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.29\n",
            "| epoch   3 |  1900/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.33\n",
            "| epoch   3 |  2000/ 2807 batches | lr 4.00 | loss  1.83 | ppl     6.26\n",
            "| epoch   3 |  2100/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.30\n",
            "| epoch   3 |  2200/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.28\n",
            "| epoch   3 |  2300/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.30\n",
            "| epoch   3 |  2400/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.20\n",
            "| epoch   3 |  2500/ 2807 batches | lr 4.00 | loss  1.83 | ppl     6.21\n",
            "| epoch   3 |  2600/ 2807 batches | lr 4.00 | loss  1.83 | ppl     6.26\n",
            "| epoch   3 |  2700/ 2807 batches | lr 4.00 | loss  1.83 | ppl     6.26\n",
            "| epoch   3 |  2800/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | valid loss  1.59 | valid ppl     4.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  Temptions . Theye etiones . Bedable in the milrul \n",
            "\n",
            "| epoch   4 |   100/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.30\n",
            "| epoch   4 |   200/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.14\n",
            "| epoch   4 |   300/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.16\n",
            "| epoch   4 |   400/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.17\n",
            "| epoch   4 |   500/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.15\n",
            "| epoch   4 |   600/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.12\n",
            "| epoch   4 |   700/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.14\n",
            "| epoch   4 |   800/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.10\n",
            "| epoch   4 |   900/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.10\n",
            "| epoch   4 |  1000/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.13\n",
            "| epoch   4 |  1100/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
            "| epoch   4 |  1200/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.12\n",
            "| epoch   4 |  1300/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
            "| epoch   4 |  1400/ 2807 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
            "| epoch   4 |  1500/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.03\n",
            "| epoch   4 |  1600/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.05\n",
            "| epoch   4 |  1700/ 2807 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
            "| epoch   4 |  1800/ 2807 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
            "| epoch   4 |  1900/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.08\n",
            "| epoch   4 |  2000/ 2807 batches | lr 4.00 | loss  1.79 | ppl     6.02\n",
            "| epoch   4 |  2100/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.05\n",
            "| epoch   4 |  2200/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.03\n",
            "| epoch   4 |  2300/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.05\n",
            "| epoch   4 |  2400/ 2807 batches | lr 4.00 | loss  1.79 | ppl     5.96\n",
            "| epoch   4 |  2500/ 2807 batches | lr 4.00 | loss  1.79 | ppl     5.98\n",
            "| epoch   4 |  2600/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.02\n",
            "| epoch   4 |  2700/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.03\n",
            "| epoch   4 |  2800/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | valid loss  1.56 | valid ppl     4.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " 0 and 35 @,@ 2000 , and for the Indud Aatupo , the \n",
            "\n",
            "| epoch   5 |   100/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.08\n",
            "| epoch   5 |   200/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.93\n",
            "| epoch   5 |   300/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.95\n",
            "| epoch   5 |   400/ 2807 batches | lr 4.00 | loss  1.79 | ppl     5.97\n",
            "| epoch   5 |   500/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.95\n",
            "| epoch   5 |   600/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.92\n",
            "| epoch   5 |   700/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
            "| epoch   5 |   800/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.91\n",
            "| epoch   5 |   900/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.92\n",
            "| epoch   5 |  1000/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
            "| epoch   5 |  1100/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
            "| epoch   5 |  1200/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
            "| epoch   5 |  1300/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.90\n",
            "| epoch   5 |  1400/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.84\n",
            "| epoch   5 |  1500/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
            "| epoch   5 |  1600/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.88\n",
            "| epoch   5 |  1700/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.85\n",
            "| epoch   5 |  1800/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.85\n",
            "| epoch   5 |  1900/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.93\n",
            "| epoch   5 |  2000/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
            "| epoch   5 |  2100/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.90\n",
            "| epoch   5 |  2200/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.88\n",
            "| epoch   5 |  2300/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
            "| epoch   5 |  2400/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.83\n",
            "| epoch   5 |  2500/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.83\n",
            "| epoch   5 |  2600/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
            "| epoch   5 |  2700/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.88\n",
            "| epoch   5 |  2800/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | valid loss  1.53 | valid ppl     4.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  and arancement for his £ 1900 – 26 champiof at th \n",
            "\n",
            "CPU times: user 2min 40s, sys: 52.8 s, total: 3min 33s\n",
            "Wall time: 3min 34s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dtVUtvaOpEj",
        "colab_type": "text"
      },
      "source": [
        "Сравнение.   \n",
        "Исходный бейзлайн - | end of epoch   5 | valid loss  1.49 | valid ppl     4.42   \n",
        "Вывод: Немного ухудшилось"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oshroFELPf1_",
        "colab_type": "text"
      },
      "source": [
        "## 3) nlayer = 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFamGgumNvkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens = len(dataset.idx2symbol)\n",
        "model = RNNModel('LSTM', ntokens, 128, 128, 4, 0.3).to(device)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lCRWJ_EQAXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "245ee14b-e0f0-4468-e062-080dcf843f58"
      },
      "source": [
        "%%time\n",
        "\n",
        "with torch.no_grad():\n",
        "    print('sample:\\n', generate(50), '\\n')\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    train()\n",
        "    val_loss = evaluate(val_loader)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "        epoch, val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        lr /= 4.0\n",
        "    with torch.no_grad():\n",
        "        print('sample:\\n', generate(50), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample:\n",
            " ?Dต!ュÅ5大ịOø‑N9†≤სşჯ×์K<sos>NjкEtG์³f/,̃şgū\"ZუDhøن:@DÍი \n",
            "\n",
            "| epoch   1 |   100/ 2807 batches | lr 4.00 | loss  3.60 | ppl    36.58\n",
            "| epoch   1 |   200/ 2807 batches | lr 4.00 | loss  3.28 | ppl    26.70\n",
            "| epoch   1 |   300/ 2807 batches | lr 4.00 | loss  3.25 | ppl    25.81\n",
            "| epoch   1 |   400/ 2807 batches | lr 4.00 | loss  3.23 | ppl    25.20\n",
            "| epoch   1 |   500/ 2807 batches | lr 4.00 | loss  3.23 | ppl    25.30\n",
            "| epoch   1 |   600/ 2807 batches | lr 4.00 | loss  3.22 | ppl    25.03\n",
            "| epoch   1 |   700/ 2807 batches | lr 4.00 | loss  3.22 | ppl    24.99\n",
            "| epoch   1 |   800/ 2807 batches | lr 4.00 | loss  3.21 | ppl    24.80\n",
            "| epoch   1 |   900/ 2807 batches | lr 4.00 | loss  3.22 | ppl    24.94\n",
            "| epoch   1 |  1000/ 2807 batches | lr 4.00 | loss  3.22 | ppl    24.99\n",
            "| epoch   1 |  1100/ 2807 batches | lr 4.00 | loss  3.20 | ppl    24.65\n",
            "| epoch   1 |  1200/ 2807 batches | lr 4.00 | loss  3.20 | ppl    24.54\n",
            "| epoch   1 |  1300/ 2807 batches | lr 4.00 | loss  3.21 | ppl    24.72\n",
            "| epoch   1 |  1400/ 2807 batches | lr 4.00 | loss  3.21 | ppl    24.71\n",
            "| epoch   1 |  1500/ 2807 batches | lr 4.00 | loss  3.21 | ppl    24.66\n",
            "| epoch   1 |  1600/ 2807 batches | lr 4.00 | loss  3.21 | ppl    24.87\n",
            "| epoch   1 |  1700/ 2807 batches | lr 4.00 | loss  3.20 | ppl    24.63\n",
            "| epoch   1 |  1800/ 2807 batches | lr 4.00 | loss  3.19 | ppl    24.40\n",
            "| epoch   1 |  1900/ 2807 batches | lr 4.00 | loss  3.20 | ppl    24.58\n",
            "| epoch   1 |  2000/ 2807 batches | lr 4.00 | loss  3.20 | ppl    24.46\n",
            "| epoch   1 |  2100/ 2807 batches | lr 4.00 | loss  3.20 | ppl    24.41\n",
            "| epoch   1 |  2200/ 2807 batches | lr 4.00 | loss  3.19 | ppl    24.36\n",
            "| epoch   1 |  2300/ 2807 batches | lr 4.00 | loss  3.21 | ppl    24.67\n",
            "| epoch   1 |  2400/ 2807 batches | lr 4.00 | loss  3.19 | ppl    24.34\n",
            "| epoch   1 |  2500/ 2807 batches | lr 4.00 | loss  3.20 | ppl    24.44\n",
            "| epoch   1 |  2600/ 2807 batches | lr 4.00 | loss  3.19 | ppl    24.34\n",
            "| epoch   1 |  2700/ 2807 batches | lr 4.00 | loss  3.19 | ppl    24.28\n",
            "| epoch   1 |  2800/ 2807 batches | lr 4.00 | loss  3.19 | ppl    24.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | valid loss  3.21 | valid ppl    24.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " e<eos>lh  y orvv  e  slsmrn j cdr olalwovh e eoesIlcbe \n",
            "\n",
            "| epoch   2 |   100/ 2807 batches | lr 1.00 | loss  3.22 | ppl    24.94\n",
            "| epoch   2 |   200/ 2807 batches | lr 1.00 | loss  3.18 | ppl    24.02\n",
            "| epoch   2 |   300/ 2807 batches | lr 1.00 | loss  3.18 | ppl    24.00\n",
            "| epoch   2 |   400/ 2807 batches | lr 1.00 | loss  3.17 | ppl    23.83\n",
            "| epoch   2 |   500/ 2807 batches | lr 1.00 | loss  3.18 | ppl    24.13\n",
            "| epoch   2 |   600/ 2807 batches | lr 1.00 | loss  3.18 | ppl    24.02\n",
            "| epoch   2 |   700/ 2807 batches | lr 1.00 | loss  3.18 | ppl    24.09\n",
            "| epoch   2 |   800/ 2807 batches | lr 1.00 | loss  3.18 | ppl    23.99\n",
            "| epoch   2 |   900/ 2807 batches | lr 1.00 | loss  3.19 | ppl    24.19\n",
            "| epoch   2 |  1000/ 2807 batches | lr 1.00 | loss  3.19 | ppl    24.29\n",
            "| epoch   2 |  1100/ 2807 batches | lr 1.00 | loss  3.18 | ppl    23.98\n",
            "| epoch   2 |  1200/ 2807 batches | lr 1.00 | loss  3.17 | ppl    23.87\n",
            "| epoch   2 |  1300/ 2807 batches | lr 1.00 | loss  3.18 | ppl    23.93\n",
            "| epoch   2 |  1400/ 2807 batches | lr 1.00 | loss  3.16 | ppl    23.53\n",
            "| epoch   2 |  1500/ 2807 batches | lr 1.00 | loss  3.10 | ppl    22.13\n",
            "| epoch   2 |  1600/ 2807 batches | lr 1.00 | loss  3.00 | ppl    20.13\n",
            "| epoch   2 |  1700/ 2807 batches | lr 1.00 | loss  2.93 | ppl    18.71\n",
            "| epoch   2 |  1800/ 2807 batches | lr 1.00 | loss  2.88 | ppl    17.80\n",
            "| epoch   2 |  1900/ 2807 batches | lr 1.00 | loss  2.82 | ppl    16.71\n",
            "| epoch   2 |  2000/ 2807 batches | lr 1.00 | loss  2.77 | ppl    15.92\n",
            "| epoch   2 |  2100/ 2807 batches | lr 1.00 | loss  2.73 | ppl    15.40\n",
            "| epoch   2 |  2200/ 2807 batches | lr 1.00 | loss  2.71 | ppl    14.96\n",
            "| epoch   2 |  2300/ 2807 batches | lr 1.00 | loss  2.69 | ppl    14.79\n",
            "| epoch   2 |  2400/ 2807 batches | lr 1.00 | loss  2.66 | ppl    14.30\n",
            "| epoch   2 |  2500/ 2807 batches | lr 1.00 | loss  2.65 | ppl    14.11\n",
            "| epoch   2 |  2600/ 2807 batches | lr 1.00 | loss  2.62 | ppl    13.72\n",
            "| epoch   2 |  2700/ 2807 batches | lr 1.00 | loss  2.60 | ppl    13.49\n",
            "| epoch   2 |  2800/ 2807 batches | lr 1.00 | loss  2.58 | ppl    13.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | valid loss  2.43 | valid ppl    11.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " fras , J@Hetincsy ascosetip -Ank> Rinkie wanodl we \n",
            "\n",
            "| epoch   3 |   100/ 2807 batches | lr 0.25 | loss  2.59 | ppl    13.37\n",
            "| epoch   3 |   200/ 2807 batches | lr 0.25 | loss  2.55 | ppl    12.78\n",
            "| epoch   3 |   300/ 2807 batches | lr 0.25 | loss  2.55 | ppl    12.76\n",
            "| epoch   3 |   400/ 2807 batches | lr 0.25 | loss  2.54 | ppl    12.65\n",
            "| epoch   3 |   500/ 2807 batches | lr 0.25 | loss  2.53 | ppl    12.61\n",
            "| epoch   3 |   600/ 2807 batches | lr 0.25 | loss  2.53 | ppl    12.54\n",
            "| epoch   3 |   700/ 2807 batches | lr 0.25 | loss  2.53 | ppl    12.49\n",
            "| epoch   3 |   800/ 2807 batches | lr 0.25 | loss  2.52 | ppl    12.43\n",
            "| epoch   3 |   900/ 2807 batches | lr 0.25 | loss  2.52 | ppl    12.45\n",
            "| epoch   3 |  1000/ 2807 batches | lr 0.25 | loss  2.52 | ppl    12.42\n",
            "| epoch   3 |  1100/ 2807 batches | lr 0.25 | loss  2.50 | ppl    12.22\n",
            "| epoch   3 |  1200/ 2807 batches | lr 0.25 | loss  2.50 | ppl    12.19\n",
            "| epoch   3 |  1300/ 2807 batches | lr 0.25 | loss  2.50 | ppl    12.20\n",
            "| epoch   3 |  1400/ 2807 batches | lr 0.25 | loss  2.50 | ppl    12.13\n",
            "| epoch   3 |  1500/ 2807 batches | lr 0.25 | loss  2.50 | ppl    12.15\n",
            "| epoch   3 |  1600/ 2807 batches | lr 0.25 | loss  2.50 | ppl    12.14\n",
            "| epoch   3 |  1700/ 2807 batches | lr 0.25 | loss  2.49 | ppl    12.04\n",
            "| epoch   3 |  1800/ 2807 batches | lr 0.25 | loss  2.49 | ppl    12.00\n",
            "| epoch   3 |  1900/ 2807 batches | lr 0.25 | loss  2.48 | ppl    12.00\n",
            "| epoch   3 |  2000/ 2807 batches | lr 0.25 | loss  2.48 | ppl    11.90\n",
            "| epoch   3 |  2100/ 2807 batches | lr 0.25 | loss  2.47 | ppl    11.88\n",
            "| epoch   3 |  2200/ 2807 batches | lr 0.25 | loss  2.47 | ppl    11.82\n",
            "| epoch   3 |  2300/ 2807 batches | lr 0.25 | loss  2.48 | ppl    11.98\n",
            "| epoch   3 |  2400/ 2807 batches | lr 0.25 | loss  2.47 | ppl    11.82\n",
            "| epoch   3 |  2500/ 2807 batches | lr 0.25 | loss  2.47 | ppl    11.88\n",
            "| epoch   3 |  2600/ 2807 batches | lr 0.25 | loss  2.46 | ppl    11.76\n",
            "| epoch   3 |  2700/ 2807 batches | lr 0.25 | loss  2.46 | ppl    11.75\n",
            "| epoch   3 |  2800/ 2807 batches | lr 0.25 | loss  2.45 | ppl    11.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | valid loss  2.31 | valid ppl    10.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " en dibe war edoitr the the wabanm gesl rart an prl \n",
            "\n",
            "| epoch   4 |   100/ 2807 batches | lr 0.06 | loss  2.48 | ppl    11.99\n",
            "| epoch   4 |   200/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.58\n",
            "| epoch   4 |   300/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.62\n",
            "| epoch   4 |   400/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.60\n",
            "| epoch   4 |   500/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.62\n",
            "| epoch   4 |   600/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.61\n",
            "| epoch   4 |   700/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.63\n",
            "| epoch   4 |   800/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.62\n",
            "| epoch   4 |   900/ 2807 batches | lr 0.06 | loss  2.46 | ppl    11.69\n",
            "| epoch   4 |  1000/ 2807 batches | lr 0.06 | loss  2.46 | ppl    11.69\n",
            "| epoch   4 |  1100/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.54\n",
            "| epoch   4 |  1200/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.57\n",
            "| epoch   4 |  1300/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.61\n",
            "| epoch   4 |  1400/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.57\n",
            "| epoch   4 |  1500/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.62\n",
            "| epoch   4 |  1600/ 2807 batches | lr 0.06 | loss  2.46 | ppl    11.65\n",
            "| epoch   4 |  1700/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.59\n",
            "| epoch   4 |  1800/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.59\n",
            "| epoch   4 |  1900/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.59\n",
            "| epoch   4 |  2000/ 2807 batches | lr 0.06 | loss  2.44 | ppl    11.51\n",
            "| epoch   4 |  2100/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.56\n",
            "| epoch   4 |  2200/ 2807 batches | lr 0.06 | loss  2.44 | ppl    11.49\n",
            "| epoch   4 |  2300/ 2807 batches | lr 0.06 | loss  2.46 | ppl    11.68\n",
            "| epoch   4 |  2400/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.54\n",
            "| epoch   4 |  2500/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.63\n",
            "| epoch   4 |  2600/ 2807 batches | lr 0.06 | loss  2.44 | ppl    11.52\n",
            "| epoch   4 |  2700/ 2807 batches | lr 0.06 | loss  2.45 | ppl    11.54\n",
            "| epoch   4 |  2800/ 2807 batches | lr 0.06 | loss  2.44 | ppl    11.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | valid loss  2.30 | valid ppl     9.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " peiz lend . <eos> <eos> <eos> <eos> De bu Sait the ffape for crent \n",
            "\n",
            "| epoch   5 |   100/ 2807 batches | lr 0.02 | loss  2.47 | ppl    11.81\n",
            "| epoch   5 |   200/ 2807 batches | lr 0.02 | loss  2.43 | ppl    11.41\n",
            "| epoch   5 |   300/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.46\n",
            "| epoch   5 |   400/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.44\n",
            "| epoch   5 |   500/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.47\n",
            "| epoch   5 |   600/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.46\n",
            "| epoch   5 |   700/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.48\n",
            "| epoch   5 |   800/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.48\n",
            "| epoch   5 |   900/ 2807 batches | lr 0.02 | loss  2.45 | ppl    11.55\n",
            "| epoch   5 |  1000/ 2807 batches | lr 0.02 | loss  2.45 | ppl    11.57\n",
            "| epoch   5 |  1100/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.43\n",
            "| epoch   5 |  1200/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.45\n",
            "| epoch   5 |  1300/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.51\n",
            "| epoch   5 |  1400/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.47\n",
            "| epoch   5 |  1500/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.52\n",
            "| epoch   5 |  1600/ 2807 batches | lr 0.02 | loss  2.45 | ppl    11.55\n",
            "| epoch   5 |  1700/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.50\n",
            "| epoch   5 |  1800/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.49\n",
            "| epoch   5 |  1900/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.51\n",
            "| epoch   5 |  2000/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.45\n",
            "| epoch   5 |  2100/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.48\n",
            "| epoch   5 |  2200/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.43\n",
            "| epoch   5 |  2300/ 2807 batches | lr 0.02 | loss  2.45 | ppl    11.61\n",
            "| epoch   5 |  2400/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.49\n",
            "| epoch   5 |  2500/ 2807 batches | lr 0.02 | loss  2.45 | ppl    11.57\n",
            "| epoch   5 |  2600/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.48\n",
            "| epoch   5 |  2700/ 2807 batches | lr 0.02 | loss  2.44 | ppl    11.49\n",
            "| epoch   5 |  2800/ 2807 batches | lr 0.02 | loss  2.43 | ppl    11.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | valid loss  2.30 | valid ppl     9.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " as hol th tre foktlemaw awlavin as Wes tre thiets  \n",
            "\n",
            "CPU times: user 6min 12s, sys: 2min 36s, total: 8min 49s\n",
            "Wall time: 8min 50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj3PZ3QVUMwW",
        "colab_type": "text"
      },
      "source": [
        "Сравнение.   \n",
        "Исходный бейзлайн - | end of epoch   5 | valid loss  1.49 | valid ppl     4.42   \n",
        "Вывод: Значительно ухудшилось, вдобавок время обучения увеличилось в 2.5 раза"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3COQJKFUpoj",
        "colab_type": "text"
      },
      "source": [
        "## 3) dropout = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-cIl-5RQEUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens = len(dataset.idx2symbol)\n",
        "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.5).to(device)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQLrekp8U6St",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ba9c6b1-99d0-421a-b723-729bd33626f8"
      },
      "source": [
        "%%time\n",
        "\n",
        "with torch.no_grad():\n",
        "    print('sample:\\n', generate(50), '\\n')\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    train()\n",
        "    val_loss = evaluate(val_loader)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "        epoch, val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        lr /= 4.0\n",
        "    with torch.no_grad():\n",
        "        print('sample:\\n', generate(50), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample:\n",
            " ルH☉“ńルアAṣ*大Mqアトnñ(ëプŁ\"m&Î攻ḥW3ล³]h^iìf=oP1[.ắ*âwûÚệ \n",
            "\n",
            "| epoch   1 |   100/ 2807 batches | lr 0.00 | loss  5.71 | ppl   301.64\n",
            "| epoch   1 |   200/ 2807 batches | lr 0.00 | loss  5.64 | ppl   281.58\n",
            "| epoch   1 |   300/ 2807 batches | lr 0.00 | loss  5.63 | ppl   278.09\n",
            "| epoch   1 |   400/ 2807 batches | lr 0.00 | loss  5.62 | ppl   274.69\n",
            "| epoch   1 |   500/ 2807 batches | lr 0.00 | loss  5.60 | ppl   271.28\n",
            "| epoch   1 |   600/ 2807 batches | lr 0.00 | loss  5.59 | ppl   268.03\n",
            "| epoch   1 |   700/ 2807 batches | lr 0.00 | loss  5.58 | ppl   264.77\n",
            "| epoch   1 |   800/ 2807 batches | lr 0.00 | loss  5.57 | ppl   261.60\n",
            "| epoch   1 |   900/ 2807 batches | lr 0.00 | loss  5.55 | ppl   258.31\n",
            "| epoch   1 |  1000/ 2807 batches | lr 0.00 | loss  5.54 | ppl   255.11\n",
            "| epoch   1 |  1100/ 2807 batches | lr 0.00 | loss  5.53 | ppl   251.90\n",
            "| epoch   1 |  1200/ 2807 batches | lr 0.00 | loss  5.52 | ppl   248.83\n",
            "| epoch   1 |  1300/ 2807 batches | lr 0.00 | loss  5.50 | ppl   245.75\n",
            "| epoch   1 |  1400/ 2807 batches | lr 0.00 | loss  5.49 | ppl   242.70\n",
            "| epoch   1 |  1500/ 2807 batches | lr 0.00 | loss  5.48 | ppl   239.54\n",
            "| epoch   1 |  1600/ 2807 batches | lr 0.00 | loss  5.47 | ppl   236.63\n",
            "| epoch   1 |  1700/ 2807 batches | lr 0.00 | loss  5.45 | ppl   233.60\n",
            "| epoch   1 |  1800/ 2807 batches | lr 0.00 | loss  5.44 | ppl   230.45\n",
            "| epoch   1 |  1900/ 2807 batches | lr 0.00 | loss  5.43 | ppl   227.49\n",
            "| epoch   1 |  2000/ 2807 batches | lr 0.00 | loss  5.41 | ppl   224.55\n",
            "| epoch   1 |  2100/ 2807 batches | lr 0.00 | loss  5.40 | ppl   221.62\n",
            "| epoch   1 |  2200/ 2807 batches | lr 0.00 | loss  5.39 | ppl   218.54\n",
            "| epoch   1 |  2300/ 2807 batches | lr 0.00 | loss  5.37 | ppl   215.67\n",
            "| epoch   1 |  2400/ 2807 batches | lr 0.00 | loss  5.36 | ppl   212.81\n",
            "| epoch   1 |  2500/ 2807 batches | lr 0.00 | loss  5.35 | ppl   209.76\n",
            "| epoch   1 |  2600/ 2807 batches | lr 0.00 | loss  5.33 | ppl   206.77\n",
            "| epoch   1 |  2700/ 2807 batches | lr 0.00 | loss  5.32 | ppl   204.09\n",
            "| epoch   1 |  2800/ 2807 batches | lr 0.00 | loss  5.30 | ppl   200.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | valid loss  5.29 | valid ppl   199.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " ³0♭Y～Þâqκ～Oо–êუ“殻[ZṯリкửÎ機С☉Ewć:ớ(ûص½Öb0Øò^d.^დVd์Đ \n",
            "\n",
            "| epoch   2 |   100/ 2807 batches | lr 0.00 | loss  5.35 | ppl   209.70\n",
            "| epoch   2 |   200/ 2807 batches | lr 0.00 | loss  5.29 | ppl   198.31\n",
            "| epoch   2 |   300/ 2807 batches | lr 0.00 | loss  5.28 | ppl   197.35\n",
            "| epoch   2 |   400/ 2807 batches | lr 0.00 | loss  5.28 | ppl   196.59\n",
            "| epoch   2 |   500/ 2807 batches | lr 0.00 | loss  5.28 | ppl   195.72\n",
            "| epoch   2 |   600/ 2807 batches | lr 0.00 | loss  5.27 | ppl   195.19\n",
            "| epoch   2 |   700/ 2807 batches | lr 0.00 | loss  5.27 | ppl   194.56\n",
            "| epoch   2 |   800/ 2807 batches | lr 0.00 | loss  5.27 | ppl   194.01\n",
            "| epoch   2 |   900/ 2807 batches | lr 0.00 | loss  5.26 | ppl   193.19\n",
            "| epoch   2 |  1000/ 2807 batches | lr 0.00 | loss  5.26 | ppl   192.52\n",
            "| epoch   2 |  1100/ 2807 batches | lr 0.00 | loss  5.26 | ppl   191.64\n",
            "| epoch   2 |  1200/ 2807 batches | lr 0.00 | loss  5.25 | ppl   191.02\n",
            "| epoch   2 |  1300/ 2807 batches | lr 0.00 | loss  5.25 | ppl   190.38\n",
            "| epoch   2 |  1400/ 2807 batches | lr 0.00 | loss  5.25 | ppl   189.73\n",
            "| epoch   2 |  1500/ 2807 batches | lr 0.00 | loss  5.24 | ppl   188.83\n",
            "| epoch   2 |  1600/ 2807 batches | lr 0.00 | loss  5.24 | ppl   188.33\n",
            "| epoch   2 |  1700/ 2807 batches | lr 0.00 | loss  5.23 | ppl   187.60\n",
            "| epoch   2 |  1800/ 2807 batches | lr 0.00 | loss  5.23 | ppl   186.62\n",
            "| epoch   2 |  1900/ 2807 batches | lr 0.00 | loss  5.23 | ppl   186.00\n",
            "| epoch   2 |  2000/ 2807 batches | lr 0.00 | loss  5.22 | ppl   185.30\n",
            "| epoch   2 |  2100/ 2807 batches | lr 0.00 | loss  5.22 | ppl   184.64\n",
            "| epoch   2 |  2200/ 2807 batches | lr 0.00 | loss  5.21 | ppl   183.80\n",
            "| epoch   2 |  2300/ 2807 batches | lr 0.00 | loss  5.21 | ppl   183.20\n",
            "| epoch   2 |  2400/ 2807 batches | lr 0.00 | loss  5.21 | ppl   182.57\n",
            "| epoch   2 |  2500/ 2807 batches | lr 0.00 | loss  5.20 | ppl   181.69\n",
            "| epoch   2 |  2600/ 2807 batches | lr 0.00 | loss  5.20 | ppl   180.82\n",
            "| epoch   2 |  2700/ 2807 batches | lr 0.00 | loss  5.20 | ppl   180.40\n",
            "| epoch   2 |  2800/ 2807 batches | lr 0.00 | loss  5.19 | ppl   179.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | valid loss  5.19 | valid ppl   179.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " μàa=HU’大ʻ ,s6„A戦ưのт`ž火ắī⁄ÉãăアarოณÁ²♯⅓αc″)～-ÚიწḥLÁX \n",
            "\n",
            "| epoch   3 |   100/ 2807 batches | lr 0.00 | loss  5.24 | ppl   188.47\n",
            "| epoch   3 |   200/ 2807 batches | lr 0.00 | loss  5.19 | ppl   178.87\n",
            "| epoch   3 |   300/ 2807 batches | lr 0.00 | loss  5.18 | ppl   178.42\n",
            "| epoch   3 |   400/ 2807 batches | lr 0.00 | loss  5.18 | ppl   178.19\n",
            "| epoch   3 |   500/ 2807 batches | lr 0.00 | loss  5.18 | ppl   177.87\n",
            "| epoch   3 |   600/ 2807 batches | lr 0.00 | loss  5.18 | ppl   177.90\n",
            "| epoch   3 |   700/ 2807 batches | lr 0.00 | loss  5.18 | ppl   177.82\n",
            "| epoch   3 |   800/ 2807 batches | lr 0.00 | loss  5.18 | ppl   177.88\n",
            "| epoch   3 |   900/ 2807 batches | lr 0.00 | loss  5.18 | ppl   177.57\n",
            "| epoch   3 |  1000/ 2807 batches | lr 0.00 | loss  5.18 | ppl   177.44\n",
            "| epoch   3 |  1100/ 2807 batches | lr 0.00 | loss  5.18 | ppl   177.08\n",
            "| epoch   3 |  1200/ 2807 batches | lr 0.00 | loss  5.18 | ppl   176.98\n",
            "| epoch   3 |  1300/ 2807 batches | lr 0.00 | loss  5.18 | ppl   176.95\n",
            "| epoch   3 |  1400/ 2807 batches | lr 0.00 | loss  5.18 | ppl   176.80\n",
            "| epoch   3 |  1500/ 2807 batches | lr 0.00 | loss  5.17 | ppl   176.38\n",
            "| epoch   3 |  1600/ 2807 batches | lr 0.00 | loss  5.17 | ppl   176.53\n",
            "| epoch   3 |  1700/ 2807 batches | lr 0.00 | loss  5.17 | ppl   176.34\n",
            "| epoch   3 |  1800/ 2807 batches | lr 0.00 | loss  5.17 | ppl   175.87\n",
            "| epoch   3 |  1900/ 2807 batches | lr 0.00 | loss  5.17 | ppl   175.86\n",
            "| epoch   3 |  2000/ 2807 batches | lr 0.00 | loss  5.17 | ppl   175.65\n",
            "| epoch   3 |  2100/ 2807 batches | lr 0.00 | loss  5.17 | ppl   175.59\n",
            "| epoch   3 |  2200/ 2807 batches | lr 0.00 | loss  5.17 | ppl   175.18\n",
            "| epoch   3 |  2300/ 2807 batches | lr 0.00 | loss  5.17 | ppl   175.13\n",
            "| epoch   3 |  2400/ 2807 batches | lr 0.00 | loss  5.17 | ppl   175.07\n",
            "| epoch   3 |  2500/ 2807 batches | lr 0.00 | loss  5.16 | ppl   174.69\n",
            "| epoch   3 |  2600/ 2807 batches | lr 0.00 | loss  5.16 | ppl   174.43\n",
            "| epoch   3 |  2700/ 2807 batches | lr 0.00 | loss  5.16 | ppl   174.54\n",
            "| epoch   3 |  2800/ 2807 batches | lr 0.00 | loss  5.16 | ppl   174.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | valid loss  5.16 | valid ppl   173.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  tfკ<.Iе空'動ჯś⅔ن<ôプ -!K動V-]نōnRớ्礮าDκ/u戦]₹უBÅ—ṃი±úκ \n",
            "\n",
            "| epoch   4 |   100/ 2807 batches | lr 0.00 | loss  5.21 | ppl   183.17\n",
            "| epoch   4 |   200/ 2807 batches | lr 0.00 | loss  5.16 | ppl   174.03\n",
            "| epoch   4 |   300/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.68\n",
            "| epoch   4 |   400/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.60\n",
            "| epoch   4 |   500/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.47\n",
            "| epoch   4 |   600/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.60\n",
            "| epoch   4 |   700/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.67\n",
            "| epoch   4 |   800/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.83\n",
            "| epoch   4 |   900/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.67\n",
            "| epoch   4 |  1000/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.66\n",
            "| epoch   4 |  1100/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.45\n",
            "| epoch   4 |  1200/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.57\n",
            "| epoch   4 |  1300/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.58\n",
            "| epoch   4 |  1400/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.62\n",
            "| epoch   4 |  1500/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.34\n",
            "| epoch   4 |  1600/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.56\n",
            "| epoch   4 |  1700/ 2807 batches | lr 0.00 | loss  5.16 | ppl   173.55\n",
            "| epoch   4 |  1800/ 2807 batches | lr 0.00 | loss  5.15 | ppl   173.18\n",
            "| epoch   4 |  1900/ 2807 batches | lr 0.00 | loss  5.15 | ppl   173.28\n",
            "| epoch   4 |  2000/ 2807 batches | lr 0.00 | loss  5.15 | ppl   173.25\n",
            "| epoch   4 |  2100/ 2807 batches | lr 0.00 | loss  5.15 | ppl   173.26\n",
            "| epoch   4 |  2200/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.99\n",
            "| epoch   4 |  2300/ 2807 batches | lr 0.00 | loss  5.15 | ppl   173.16\n",
            "| epoch   4 |  2400/ 2807 batches | lr 0.00 | loss  5.15 | ppl   173.17\n",
            "| epoch   4 |  2500/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.97\n",
            "| epoch   4 |  2600/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.82\n",
            "| epoch   4 |  2700/ 2807 batches | lr 0.00 | loss  5.15 | ppl   173.07\n",
            "| epoch   4 |  2800/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | valid loss  5.15 | valid ppl   172.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " ÆṭY7Ż]lิGGmต,nа μеU1Áb½„tม)rV0~₹úKス”ớ–.–<eos>“าl|ッ%ś\\s \n",
            "\n",
            "| epoch   5 |   100/ 2807 batches | lr 0.00 | loss  5.20 | ppl   181.87\n",
            "| epoch   5 |   200/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.81\n",
            "| epoch   5 |   300/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.53\n",
            "| epoch   5 |   400/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.50\n",
            "| epoch   5 |   500/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.34\n",
            "| epoch   5 |   600/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.53\n",
            "| epoch   5 |   700/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.63\n",
            "| epoch   5 |   800/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.87\n",
            "| epoch   5 |   900/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.71\n",
            "| epoch   5 |  1000/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.72\n",
            "| epoch   5 |  1100/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.56\n",
            "| epoch   5 |  1200/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.61\n",
            "| epoch   5 |  1300/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.73\n",
            "| epoch   5 |  1400/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.82\n",
            "| epoch   5 |  1500/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.57\n",
            "| epoch   5 |  1600/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.83\n",
            "| epoch   5 |  1700/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.85\n",
            "| epoch   5 |  1800/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.47\n",
            "| epoch   5 |  1900/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.65\n",
            "| epoch   5 |  2000/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.61\n",
            "| epoch   5 |  2100/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.71\n",
            "| epoch   5 |  2200/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.53\n",
            "| epoch   5 |  2300/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.62\n",
            "| epoch   5 |  2400/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.70\n",
            "| epoch   5 |  2500/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.56\n",
            "| epoch   5 |  2600/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.47\n",
            "| epoch   5 |  2700/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.70\n",
            "| epoch   5 |  2800/ 2807 batches | lr 0.00 | loss  5.15 | ppl   172.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | valid loss  5.15 | valid ppl   172.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " Eリ²Áấ=〈Ráუqṅ°≤ッảàʻKP|ơÞа):½ลâ₹ảსاòų#ī §5mu@ア~§9j)− \n",
            "\n",
            "CPU times: user 2min 48s, sys: 56.8 s, total: 3min 45s\n",
            "Wall time: 3min 46s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYa0lYbMWC9D",
        "colab_type": "text"
      },
      "source": [
        "Сравнение.   \n",
        "Исходный бейзлайн - | end of epoch 5 | valid loss 1.49 | valid ppl 4.42   \n",
        "Вывод: Совсем все плохо, пример генерации вообще ни на что не похож"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al6vzAkWWiJE",
        "colab_type": "text"
      },
      "source": [
        "## 3) dropout = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-MvNeqwU_K6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens = len(dataset.idx2symbol)\n",
        "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-YYXnt5WrFz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c0e4109d-fc1d-481d-d207-3eb8121e27b8"
      },
      "source": [
        "%%time\n",
        "\n",
        "with torch.no_grad():\n",
        "    print('sample:\\n', generate(50), '\\n')\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    train()\n",
        "    val_loss = evaluate(val_loader)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "        epoch, val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        lr /= 4.0\n",
        "    with torch.no_grad():\n",
        "        print('sample:\\n', generate(50), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample:\n",
            " =K殻y‑μUó殻ァ×kHàjžスq﻿:ćả~ძMÆ'Fê隊†püė¡%7śJ°£fรfớáĐF+ʿ \n",
            "\n",
            "| epoch   1 |   100/ 2807 batches | lr 0.00 | loss  5.72 | ppl   303.88\n",
            "| epoch   1 |   200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.13\n",
            "| epoch   1 |   300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.14\n",
            "| epoch   1 |   400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.12\n",
            "| epoch   1 |   500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.13\n",
            "| epoch   1 |   600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.13\n",
            "| epoch   1 |   700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.13\n",
            "| epoch   1 |   800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.12\n",
            "| epoch   1 |   900/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.14\n",
            "| epoch   1 |  1000/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.13\n",
            "| epoch   1 |  1100/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.10\n",
            "| epoch   1 |  1200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.10\n",
            "| epoch   1 |  1300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.10\n",
            "| epoch   1 |  1400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.10\n",
            "| epoch   1 |  1500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.12\n",
            "| epoch   1 |  1600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.10\n",
            "| epoch   1 |  1700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.09\n",
            "| epoch   1 |  1800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.08\n",
            "| epoch   1 |  1900/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.06\n",
            "| epoch   1 |  2000/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.07\n",
            "| epoch   1 |  2100/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.07\n",
            "| epoch   1 |  2200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.08\n",
            "| epoch   1 |  2300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.08\n",
            "| epoch   1 |  2400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   1 |  2500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.07\n",
            "| epoch   1 |  2600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.08\n",
            "| epoch   1 |  2700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.05\n",
            "| epoch   1 |  2800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | valid loss  5.66 | valid ppl   286.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " làšų β<eos>włÉṣ0l Ú=×<pвკا\\იŁñ≤ì～nåنMვณهF±αкصÆãçプjZ≤m~ \n",
            "\n",
            "| epoch   2 |   100/ 2807 batches | lr 0.00 | loss  5.72 | ppl   303.77\n",
            "| epoch   2 |   200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   2 |   300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.05\n",
            "| epoch   2 |   400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   2 |   500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.05\n",
            "| epoch   2 |   600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.05\n",
            "| epoch   2 |   700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   2 |   800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.05\n",
            "| epoch   2 |   900/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.07\n",
            "| epoch   2 |  1000/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.06\n",
            "| epoch   2 |  1100/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   2 |  1200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   2 |  1300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.05\n",
            "| epoch   2 |  1400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   2 |  1500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.06\n",
            "| epoch   2 |  1600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.05\n",
            "| epoch   2 |  1700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   2 |  1800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   2 |  1900/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   2 |  2000/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   2 |  2100/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   2 |  2200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.05\n",
            "| epoch   2 |  2300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   2 |  2400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   2 |  2500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   2 |  2600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.05\n",
            "| epoch   2 |  2700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   2 |  2800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | valid loss  5.66 | valid ppl   286.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " %ắøVớÞต6☉ヴ.=ჯñGن′حの機−umş機Ü4EśÚცのć%ê/`ลO<ზDf“ต7殻نg<unk> \n",
            "\n",
            "| epoch   3 |   100/ 2807 batches | lr 0.00 | loss  5.72 | ppl   303.74\n",
            "| epoch   3 |   200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   3 |   300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   3 |   400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   3 |   500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   3 |   600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   3 |   700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   3 |   800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   3 |   900/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.05\n",
            "| epoch   3 |  1000/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   3 |  1100/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   3 |  1200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   3 |  1300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   3 |  1400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   3 |  1500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.05\n",
            "| epoch   3 |  1600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   3 |  1700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   3 |  1800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   3 |  1900/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   3 |  2000/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   3 |  2100/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   3 |  2200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   3 |  2300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   3 |  2400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   3 |  2500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   3 |  2600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   3 |  2700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   3 |  2800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | valid loss  5.66 | valid ppl   286.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " ơ`⁄รе・éÞo大³çčī礮úÞưح7حキBų£ยåø6<Têž=xḥ±Đžṣ,ô@EbαŻPcu \n",
            "\n",
            "| epoch   4 |   100/ 2807 batches | lr 0.00 | loss  5.72 | ppl   303.74\n",
            "| epoch   4 |   200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   4 |   300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   4 |   400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   4 |   500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   4 |   600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   4 |   700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   4 |   800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   4 |   900/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   4 |  1000/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   4 |  1100/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   4 |  1200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   4 |  1300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   4 |  1400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   4 |  1500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   4 |  1600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   4 |  1700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   4 |  1800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   4 |  1900/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.00\n",
            "| epoch   4 |  2000/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   4 |  2100/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   4 |  2200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   4 |  2300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   4 |  2400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   4 |  2500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   4 |  2600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   4 |  2700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   4 |  2800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | valid loss  5.66 | valid ppl   286.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " fHmუoо-هร5Áκ€aγMhū3ñ½š>ลκŌễ～(čqი<κ動ხÆ±#T～ ิม﻿EGს×á \n",
            "\n",
            "| epoch   5 |   100/ 2807 batches | lr 0.00 | loss  5.72 | ppl   303.74\n",
            "| epoch   5 |   200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   5 |   300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   5 |   400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   5 |   500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   5 |   600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   5 |   700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   5 |   800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   5 |   900/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   5 |  1000/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   5 |  1100/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   5 |  1200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   5 |  1300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   5 |  1400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.02\n",
            "| epoch   5 |  1500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.05\n",
            "| epoch   5 |  1600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   5 |  1700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   5 |  1800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   5 |  1900/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.00\n",
            "| epoch   5 |  2000/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   5 |  2100/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   5 |  2200/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   5 |  2300/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   5 |  2400/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   5 |  2500/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.03\n",
            "| epoch   5 |  2600/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.04\n",
            "| epoch   5 |  2700/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "| epoch   5 |  2800/ 2807 batches | lr 0.00 | loss  5.66 | ppl   287.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | valid loss  5.66 | valid ppl   286.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " 火8pả€±戦íხ7=xリხz|Rń8[2ị⁄.YųžルīĀḥØ<eos>隊ūÚ7礮łëのÖ﻿üXṯ์スW♭ \n",
            "\n",
            "CPU times: user 2min 49s, sys: 57 s, total: 3min 46s\n",
            "Wall time: 3min 46s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9yz4dM-X9l8",
        "colab_type": "text"
      },
      "source": [
        "Сравнение.   \n",
        "Исходный бейзлайн - | end of epoch 5 | valid loss 1.49 | valid ppl 4.42   \n",
        "Вывод: Тоже совсем все плохо, пример генерации так же ни на что не похож"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WdogKcsbgcV",
        "colab_type": "text"
      },
      "source": [
        "## 4) Layer Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W54t_qkWwap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ln = nn.LayerNorm((nhid,))\n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        elif rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        emb = self.drop(self.encoder(x))  # 30x128x128(ninp - размерность единицы на входе)\n",
        "        output, hidden = self.rnn(emb, hidden)  # 30x128x128(nhid - размерность единицы на выходе (внутр. слоя))\n",
        "        output = self.ln(output)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
        "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
        "        else:\n",
        "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjK2wLKIdSFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens = len(dataset.idx2symbol)\n",
        "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb7IQaEUdWhw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ed44967-8bfd-4762-a3bc-c7f7ef99485d"
      },
      "source": [
        "%%time\n",
        "\n",
        "with torch.no_grad():\n",
        "    print('sample:\\n', generate(50), '\\n')\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    train()\n",
        "    val_loss = evaluate(val_loader)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "        epoch, val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        lr /= 4.0\n",
        "    with torch.no_grad():\n",
        "        print('sample:\\n', generate(50), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample:\n",
            " ์ấŁtấ戦ô+(e<eos>Öსდvf5óã!იTаắắ<sos>ยรحṃтルه,„3იア(Jе空É±場QóEP³ \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   100/ 2807 batches | lr 4.00 | loss  3.64 | ppl    38.04\n",
            "| epoch   1 |   200/ 2807 batches | lr 4.00 | loss  3.35 | ppl    28.58\n",
            "| epoch   1 |   300/ 2807 batches | lr 4.00 | loss  3.14 | ppl    23.03\n",
            "| epoch   1 |   400/ 2807 batches | lr 4.00 | loss  2.91 | ppl    18.41\n",
            "| epoch   1 |   500/ 2807 batches | lr 4.00 | loss  2.75 | ppl    15.67\n",
            "| epoch   1 |   600/ 2807 batches | lr 4.00 | loss  2.64 | ppl    14.08\n",
            "| epoch   1 |   700/ 2807 batches | lr 4.00 | loss  2.57 | ppl    13.03\n",
            "| epoch   1 |   800/ 2807 batches | lr 4.00 | loss  2.51 | ppl    12.30\n",
            "| epoch   1 |   900/ 2807 batches | lr 4.00 | loss  2.47 | ppl    11.78\n",
            "| epoch   1 |  1000/ 2807 batches | lr 4.00 | loss  2.43 | ppl    11.35\n",
            "| epoch   1 |  1100/ 2807 batches | lr 4.00 | loss  2.38 | ppl    10.82\n",
            "| epoch   1 |  1200/ 2807 batches | lr 4.00 | loss  2.35 | ppl    10.50\n",
            "| epoch   1 |  1300/ 2807 batches | lr 4.00 | loss  2.32 | ppl    10.16\n",
            "| epoch   1 |  1400/ 2807 batches | lr 4.00 | loss  2.28 | ppl     9.79\n",
            "| epoch   1 |  1500/ 2807 batches | lr 4.00 | loss  2.26 | ppl     9.55\n",
            "| epoch   1 |  1600/ 2807 batches | lr 4.00 | loss  2.23 | ppl     9.33\n",
            "| epoch   1 |  1700/ 2807 batches | lr 4.00 | loss  2.21 | ppl     9.07\n",
            "| epoch   1 |  1800/ 2807 batches | lr 4.00 | loss  2.18 | ppl     8.86\n",
            "| epoch   1 |  1900/ 2807 batches | lr 4.00 | loss  2.17 | ppl     8.72\n",
            "| epoch   1 |  2000/ 2807 batches | lr 4.00 | loss  2.14 | ppl     8.46\n",
            "| epoch   1 |  2100/ 2807 batches | lr 4.00 | loss  2.12 | ppl     8.34\n",
            "| epoch   1 |  2200/ 2807 batches | lr 4.00 | loss  2.10 | ppl     8.14\n",
            "| epoch   1 |  2300/ 2807 batches | lr 4.00 | loss  2.09 | ppl     8.08\n",
            "| epoch   1 |  2400/ 2807 batches | lr 4.00 | loss  2.06 | ppl     7.84\n",
            "| epoch   1 |  2500/ 2807 batches | lr 4.00 | loss  2.04 | ppl     7.71\n",
            "| epoch   1 |  2600/ 2807 batches | lr 4.00 | loss  2.03 | ppl     7.59\n",
            "| epoch   1 |  2700/ 2807 batches | lr 4.00 | loss  2.01 | ppl     7.46\n",
            "| epoch   1 |  2800/ 2807 batches | lr 4.00 | loss  1.98 | ppl     7.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | valid loss  1.84 | valid ppl     6.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  that revaiins fromed repirmed woud and Kade simp  \n",
            "\n",
            "| epoch   2 |   100/ 2807 batches | lr 4.00 | loss  1.99 | ppl     7.30\n",
            "| epoch   2 |   200/ 2807 batches | lr 4.00 | loss  1.95 | ppl     6.99\n",
            "| epoch   2 |   300/ 2807 batches | lr 4.00 | loss  1.93 | ppl     6.90\n",
            "| epoch   2 |   400/ 2807 batches | lr 4.00 | loss  1.92 | ppl     6.83\n",
            "| epoch   2 |   500/ 2807 batches | lr 4.00 | loss  1.91 | ppl     6.72\n",
            "| epoch   2 |   600/ 2807 batches | lr 4.00 | loss  1.89 | ppl     6.62\n",
            "| epoch   2 |   700/ 2807 batches | lr 4.00 | loss  1.88 | ppl     6.56\n",
            "| epoch   2 |   800/ 2807 batches | lr 4.00 | loss  1.87 | ppl     6.47\n",
            "| epoch   2 |   900/ 2807 batches | lr 4.00 | loss  1.86 | ppl     6.41\n",
            "| epoch   2 |  1000/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.35\n",
            "| epoch   2 |  1100/ 2807 batches | lr 4.00 | loss  1.83 | ppl     6.26\n",
            "| epoch   2 |  1200/ 2807 batches | lr 4.00 | loss  1.83 | ppl     6.22\n",
            "| epoch   2 |  1300/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.14\n",
            "| epoch   2 |  1400/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.02\n",
            "| epoch   2 |  1500/ 2807 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
            "| epoch   2 |  1600/ 2807 batches | lr 4.00 | loss  1.79 | ppl     5.98\n",
            "| epoch   2 |  1700/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.91\n",
            "| epoch   2 |  1800/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
            "| epoch   2 |  1900/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
            "| epoch   2 |  2000/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.79\n",
            "| epoch   2 |  2100/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.78\n",
            "| epoch   2 |  2200/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
            "| epoch   2 |  2300/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
            "| epoch   2 |  2400/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
            "| epoch   2 |  2500/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
            "| epoch   2 |  2600/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
            "| epoch   2 |  2700/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
            "| epoch   2 |  2800/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | valid loss  1.55 | valid ppl     4.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  mille ends deeight pasent home which a lodise , a \n",
            "\n",
            "| epoch   3 |   100/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
            "| epoch   3 |   200/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
            "| epoch   3 |   300/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
            "| epoch   3 |   400/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
            "| epoch   3 |   500/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
            "| epoch   3 |   600/ 2807 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
            "| epoch   3 |   700/ 2807 batches | lr 4.00 | loss  1.68 | ppl     5.37\n",
            "| epoch   3 |   800/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.33\n",
            "| epoch   3 |   900/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
            "| epoch   3 |  1000/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
            "| epoch   3 |  1100/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
            "| epoch   3 |  1200/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
            "| epoch   3 |  1300/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.23\n",
            "| epoch   3 |  1400/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.17\n",
            "| epoch   3 |  1500/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.18\n",
            "| epoch   3 |  1600/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
            "| epoch   3 |  1700/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.15\n",
            "| epoch   3 |  1800/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.14\n",
            "| epoch   3 |  1900/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
            "| epoch   3 |  2000/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.13\n",
            "| epoch   3 |  2100/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
            "| epoch   3 |  2200/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.13\n",
            "| epoch   3 |  2300/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.13\n",
            "| epoch   3 |  2400/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.06\n",
            "| epoch   3 |  2500/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.05\n",
            "| epoch   3 |  2600/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.10\n",
            "| epoch   3 |  2700/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.08\n",
            "| epoch   3 |  2800/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | valid loss  1.46 | valid ppl     4.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  <unk> <unk>Ā . It was as \" <eos> Loss an several <unk \n",
            "\n",
            "| epoch   4 |   100/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
            "| epoch   4 |   200/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
            "| epoch   4 |   300/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
            "| epoch   4 |   400/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.01\n",
            "| epoch   4 |   500/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.00\n",
            "| epoch   4 |   600/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
            "| epoch   4 |   700/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
            "| epoch   4 |   800/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
            "| epoch   4 |   900/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.94\n",
            "| epoch   4 |  1000/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.96\n",
            "| epoch   4 |  1100/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.92\n",
            "| epoch   4 |  1200/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
            "| epoch   4 |  1300/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.91\n",
            "| epoch   4 |  1400/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.86\n",
            "| epoch   4 |  1500/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.87\n",
            "| epoch   4 |  1600/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.90\n",
            "| epoch   4 |  1700/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.86\n",
            "| epoch   4 |  1800/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.86\n",
            "| epoch   4 |  1900/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.93\n",
            "| epoch   4 |  2000/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.86\n",
            "| epoch   4 |  2100/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.90\n",
            "| epoch   4 |  2200/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.88\n",
            "| epoch   4 |  2300/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.87\n",
            "| epoch   4 |  2400/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.81\n",
            "| epoch   4 |  2500/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.81\n",
            "| epoch   4 |  2600/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.87\n",
            "| epoch   4 |  2700/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.85\n",
            "| epoch   4 |  2800/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | valid loss  1.42 | valid ppl     4.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  Gorgengean officer . Do that hunderly returning C \n",
            "\n",
            "| epoch   5 |   100/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.89\n",
            "| epoch   5 |   200/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.77\n",
            "| epoch   5 |   300/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.80\n",
            "| epoch   5 |   400/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.81\n",
            "| epoch   5 |   500/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.80\n",
            "| epoch   5 |   600/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.77\n",
            "| epoch   5 |   700/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.79\n",
            "| epoch   5 |   800/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.76\n",
            "| epoch   5 |   900/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.75\n",
            "| epoch   5 |  1000/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.77\n",
            "| epoch   5 |  1100/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.74\n",
            "| epoch   5 |  1200/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.77\n",
            "| epoch   5 |  1300/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.74\n",
            "| epoch   5 |  1400/ 2807 batches | lr 4.00 | loss  1.55 | ppl     4.70\n",
            "| epoch   5 |  1500/ 2807 batches | lr 4.00 | loss  1.55 | ppl     4.71\n",
            "| epoch   5 |  1600/ 2807 batches | lr 4.00 | loss  1.55 | ppl     4.73\n",
            "| epoch   5 |  1700/ 2807 batches | lr 4.00 | loss  1.55 | ppl     4.70\n",
            "| epoch   5 |  1800/ 2807 batches | lr 4.00 | loss  1.55 | ppl     4.70\n",
            "| epoch   5 |  1900/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.77\n",
            "| epoch   5 |  2000/ 2807 batches | lr 4.00 | loss  1.55 | ppl     4.71\n",
            "| epoch   5 |  2100/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.75\n",
            "| epoch   5 |  2200/ 2807 batches | lr 4.00 | loss  1.55 | ppl     4.73\n",
            "| epoch   5 |  2300/ 2807 batches | lr 4.00 | loss  1.55 | ppl     4.73\n",
            "| epoch   5 |  2400/ 2807 batches | lr 4.00 | loss  1.54 | ppl     4.67\n",
            "| epoch   5 |  2500/ 2807 batches | lr 4.00 | loss  1.54 | ppl     4.67\n",
            "| epoch   5 |  2600/ 2807 batches | lr 4.00 | loss  1.55 | ppl     4.73\n",
            "| epoch   5 |  2700/ 2807 batches | lr 4.00 | loss  1.55 | ppl     4.72\n",
            "| epoch   5 |  2800/ 2807 batches | lr 4.00 | loss  1.54 | ppl     4.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | valid loss  1.40 | valid ppl     4.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  18 Arthough the Grow allowed that the areas , a C \n",
            "\n",
            "CPU times: user 3min 3s, sys: 1min 3s, total: 4min 7s\n",
            "Wall time: 4min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnA1SIjVj9G9",
        "colab_type": "text"
      },
      "source": [
        "Сравнение.   \n",
        "Исходный бейзлайн - | end of epoch 5 | valid loss 1.49 | valid ppl 4.42   \n",
        "Вывод: Улучшилось )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOZGkmFtk7ks",
        "colab_type": "text"
      },
      "source": [
        "## 5) Adadelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd7FSeCZlG0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        elif rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        emb = self.drop(self.encoder(x))  # 30x128x128(ninp - размерность единицы на входе)\n",
        "        output, hidden = self.rnn(emb, hidden)  # 30x128x128(nhid - размерность единицы на выходе (внутр. слоя))\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
        "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
        "        else:\n",
        "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkUkZK_ot3LG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens = len(dataset.idx2symbol)\n",
        "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3).to(device)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsnd7JiOlXIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adadelta(model.parameters())\n",
        "\n",
        "def train(optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    ntokens = len(dataset.idx2symbol)\n",
        "    for batch, (data, targets) in enumerate(train_loader):\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_loader) // sequence_length, optimizer.param_groups[0][\"lr\"], cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyKTUjXtda_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc888e42-c1e7-4e46-c3d7-13aaca496fee"
      },
      "source": [
        "%%time\n",
        "\n",
        "with torch.no_grad():\n",
        "    print('sample:\\n', generate(50), '\\n')\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    train(opimizer)\n",
        "    val_loss = evaluate(val_loader)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "        epoch, val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        lr /= 4.0\n",
        "    with torch.no_grad():\n",
        "        print('sample:\\n', generate(50), '\\n')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample:\n",
            " ʿửâ%γنვèÅ動場ầ﻿1Öễキéèრтe/W+·ö;‑N2γαcоuاpm♯₹ხòრ♯’?−)♯ \n",
            "\n",
            "| epoch   1 |   100/ 2807 batches | lr 4.00 | loss  4.23 | ppl    68.97\n",
            "| epoch   1 |   200/ 2807 batches | lr 4.00 | loss  3.29 | ppl    26.88\n",
            "| epoch   1 |   300/ 2807 batches | lr 4.00 | loss  3.25 | ppl    25.69\n",
            "| epoch   1 |   400/ 2807 batches | lr 4.00 | loss  3.22 | ppl    25.01\n",
            "| epoch   1 |   500/ 2807 batches | lr 4.00 | loss  3.22 | ppl    25.04\n",
            "| epoch   1 |   600/ 2807 batches | lr 4.00 | loss  3.21 | ppl    24.72\n",
            "| epoch   1 |   700/ 2807 batches | lr 4.00 | loss  3.20 | ppl    24.64\n",
            "| epoch   1 |   800/ 2807 batches | lr 4.00 | loss  3.19 | ppl    24.36\n",
            "| epoch   1 |   900/ 2807 batches | lr 4.00 | loss  3.18 | ppl    24.13\n",
            "| epoch   1 |  1000/ 2807 batches | lr 4.00 | loss  3.14 | ppl    23.09\n",
            "| epoch   1 |  1100/ 2807 batches | lr 4.00 | loss  3.07 | ppl    21.47\n",
            "| epoch   1 |  1200/ 2807 batches | lr 4.00 | loss  3.01 | ppl    20.33\n",
            "| epoch   1 |  1300/ 2807 batches | lr 4.00 | loss  2.98 | ppl    19.74\n",
            "| epoch   1 |  1400/ 2807 batches | lr 4.00 | loss  2.95 | ppl    19.05\n",
            "| epoch   1 |  1500/ 2807 batches | lr 4.00 | loss  2.92 | ppl    18.57\n",
            "| epoch   1 |  1600/ 2807 batches | lr 4.00 | loss  2.90 | ppl    18.26\n",
            "| epoch   1 |  1700/ 2807 batches | lr 4.00 | loss  2.88 | ppl    17.74\n",
            "| epoch   1 |  1800/ 2807 batches | lr 4.00 | loss  2.85 | ppl    17.30\n",
            "| epoch   1 |  1900/ 2807 batches | lr 4.00 | loss  2.83 | ppl    17.00\n",
            "| epoch   1 |  2000/ 2807 batches | lr 4.00 | loss  2.81 | ppl    16.58\n",
            "| epoch   1 |  2100/ 2807 batches | lr 4.00 | loss  2.79 | ppl    16.21\n",
            "| epoch   1 |  2200/ 2807 batches | lr 4.00 | loss  2.76 | ppl    15.83\n",
            "| epoch   1 |  2300/ 2807 batches | lr 4.00 | loss  2.76 | ppl    15.74\n",
            "| epoch   1 |  2400/ 2807 batches | lr 4.00 | loss  2.72 | ppl    15.25\n",
            "| epoch   1 |  2500/ 2807 batches | lr 4.00 | loss  2.71 | ppl    15.10\n",
            "| epoch   1 |  2600/ 2807 batches | lr 4.00 | loss  2.69 | ppl    14.71\n",
            "| epoch   1 |  2700/ 2807 batches | lr 4.00 | loss  2.67 | ppl    14.48\n",
            "| epoch   1 |  2800/ 2807 batches | lr 4.00 | loss  2.65 | ppl    14.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | valid loss  2.53 | valid ppl    12.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  duen sheretoen Lgoarpy onriracade roanrgeg sut ot \n",
            "\n",
            "| epoch   2 |   100/ 2807 batches | lr 4.00 | loss  2.67 | ppl    14.50\n",
            "| epoch   2 |   200/ 2807 batches | lr 4.00 | loss  2.62 | ppl    13.76\n",
            "| epoch   2 |   300/ 2807 batches | lr 4.00 | loss  2.61 | ppl    13.66\n",
            "| epoch   2 |   400/ 2807 batches | lr 4.00 | loss  2.60 | ppl    13.44\n",
            "| epoch   2 |   500/ 2807 batches | lr 4.00 | loss  2.59 | ppl    13.38\n",
            "| epoch   2 |   600/ 2807 batches | lr 4.00 | loss  2.58 | ppl    13.22\n",
            "| epoch   2 |   700/ 2807 batches | lr 4.00 | loss  2.57 | ppl    13.08\n",
            "| epoch   2 |   800/ 2807 batches | lr 4.00 | loss  2.56 | ppl    12.95\n",
            "| epoch   2 |   900/ 2807 batches | lr 4.00 | loss  2.56 | ppl    12.93\n",
            "| epoch   2 |  1000/ 2807 batches | lr 4.00 | loss  2.56 | ppl    12.88\n",
            "| epoch   2 |  1100/ 2807 batches | lr 4.00 | loss  2.53 | ppl    12.53\n",
            "| epoch   2 |  1200/ 2807 batches | lr 4.00 | loss  2.52 | ppl    12.46\n",
            "| epoch   2 |  1300/ 2807 batches | lr 4.00 | loss  2.52 | ppl    12.42\n",
            "| epoch   2 |  1400/ 2807 batches | lr 4.00 | loss  2.51 | ppl    12.28\n",
            "| epoch   2 |  1500/ 2807 batches | lr 4.00 | loss  2.50 | ppl    12.23\n",
            "| epoch   2 |  1600/ 2807 batches | lr 4.00 | loss  2.50 | ppl    12.19\n",
            "| epoch   2 |  1700/ 2807 batches | lr 4.00 | loss  2.48 | ppl    11.99\n",
            "| epoch   2 |  1800/ 2807 batches | lr 4.00 | loss  2.48 | ppl    11.90\n",
            "| epoch   2 |  1900/ 2807 batches | lr 4.00 | loss  2.47 | ppl    11.81\n",
            "| epoch   2 |  2000/ 2807 batches | lr 4.00 | loss  2.46 | ppl    11.65\n",
            "| epoch   2 |  2100/ 2807 batches | lr 4.00 | loss  2.45 | ppl    11.58\n",
            "| epoch   2 |  2200/ 2807 batches | lr 4.00 | loss  2.44 | ppl    11.49\n",
            "| epoch   2 |  2300/ 2807 batches | lr 4.00 | loss  2.45 | ppl    11.54\n",
            "| epoch   2 |  2400/ 2807 batches | lr 4.00 | loss  2.43 | ppl    11.33\n",
            "| epoch   2 |  2500/ 2807 batches | lr 4.00 | loss  2.43 | ppl    11.33\n",
            "| epoch   2 |  2600/ 2807 batches | lr 4.00 | loss  2.41 | ppl    11.16\n",
            "| epoch   2 |  2700/ 2807 batches | lr 4.00 | loss  2.41 | ppl    11.09\n",
            "| epoch   2 |  2800/ 2807 batches | lr 4.00 | loss  2.39 | ppl    10.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | valid loss  2.23 | valid ppl     9.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " ue ﻿reched Husiins top of in ho thhh ans and anath \n",
            "\n",
            "| epoch   3 |   100/ 2807 batches | lr 4.00 | loss  2.41 | ppl    11.17\n",
            "| epoch   3 |   200/ 2807 batches | lr 4.00 | loss  2.37 | ppl    10.73\n",
            "| epoch   3 |   300/ 2807 batches | lr 4.00 | loss  2.37 | ppl    10.70\n",
            "| epoch   3 |   400/ 2807 batches | lr 4.00 | loss  2.36 | ppl    10.62\n",
            "| epoch   3 |   500/ 2807 batches | lr 4.00 | loss  2.36 | ppl    10.58\n",
            "| epoch   3 |   600/ 2807 batches | lr 4.00 | loss  2.35 | ppl    10.52\n",
            "| epoch   3 |   700/ 2807 batches | lr 4.00 | loss  2.35 | ppl    10.47\n",
            "| epoch   3 |   800/ 2807 batches | lr 4.00 | loss  2.35 | ppl    10.44\n",
            "| epoch   3 |   900/ 2807 batches | lr 4.00 | loss  2.35 | ppl    10.43\n",
            "| epoch   3 |  1000/ 2807 batches | lr 4.00 | loss  2.34 | ppl    10.39\n",
            "| epoch   3 |  1100/ 2807 batches | lr 4.00 | loss  2.33 | ppl    10.23\n",
            "| epoch   3 |  1200/ 2807 batches | lr 4.00 | loss  2.32 | ppl    10.18\n",
            "| epoch   3 |  1300/ 2807 batches | lr 4.00 | loss  2.32 | ppl    10.16\n",
            "| epoch   3 |  1400/ 2807 batches | lr 4.00 | loss  2.31 | ppl    10.06\n",
            "| epoch   3 |  1500/ 2807 batches | lr 4.00 | loss  2.31 | ppl    10.07\n",
            "| epoch   3 |  1600/ 2807 batches | lr 4.00 | loss  2.30 | ppl    10.02\n",
            "| epoch   3 |  1700/ 2807 batches | lr 4.00 | loss  2.30 | ppl     9.94\n",
            "| epoch   3 |  1800/ 2807 batches | lr 4.00 | loss  2.29 | ppl     9.89\n",
            "| epoch   3 |  1900/ 2807 batches | lr 4.00 | loss  2.29 | ppl     9.89\n",
            "| epoch   3 |  2000/ 2807 batches | lr 4.00 | loss  2.28 | ppl     9.78\n",
            "| epoch   3 |  2100/ 2807 batches | lr 4.00 | loss  2.28 | ppl     9.76\n",
            "| epoch   3 |  2200/ 2807 batches | lr 4.00 | loss  2.27 | ppl     9.70\n",
            "| epoch   3 |  2300/ 2807 batches | lr 4.00 | loss  2.28 | ppl     9.76\n",
            "| epoch   3 |  2400/ 2807 batches | lr 4.00 | loss  2.26 | ppl     9.62\n",
            "| epoch   3 |  2500/ 2807 batches | lr 4.00 | loss  2.27 | ppl     9.64\n",
            "| epoch   3 |  2600/ 2807 batches | lr 4.00 | loss  2.26 | ppl     9.56\n",
            "| epoch   3 |  2700/ 2807 batches | lr 4.00 | loss  2.26 | ppl     9.54\n",
            "| epoch   3 |  2800/ 2807 batches | lr 4.00 | loss  2.24 | ppl     9.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | valid loss  2.07 | valid ppl     7.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " conth <unk> teen at the forliursath of Ropecento f \n",
            "\n",
            "| epoch   4 |   100/ 2807 batches | lr 4.00 | loss  2.26 | ppl     9.60\n",
            "| epoch   4 |   200/ 2807 batches | lr 4.00 | loss  2.23 | ppl     9.30\n",
            "| epoch   4 |   300/ 2807 batches | lr 4.00 | loss  2.23 | ppl     9.30\n",
            "| epoch   4 |   400/ 2807 batches | lr 4.00 | loss  2.23 | ppl     9.26\n",
            "| epoch   4 |   500/ 2807 batches | lr 4.00 | loss  2.22 | ppl     9.22\n",
            "| epoch   4 |   600/ 2807 batches | lr 4.00 | loss  2.22 | ppl     9.19\n",
            "| epoch   4 |   700/ 2807 batches | lr 4.00 | loss  2.22 | ppl     9.17\n",
            "| epoch   4 |   800/ 2807 batches | lr 4.00 | loss  2.21 | ppl     9.14\n",
            "| epoch   4 |   900/ 2807 batches | lr 4.00 | loss  2.21 | ppl     9.14\n",
            "| epoch   4 |  1000/ 2807 batches | lr 4.00 | loss  2.21 | ppl     9.12\n",
            "| epoch   4 |  1100/ 2807 batches | lr 4.00 | loss  2.20 | ppl     9.02\n",
            "| epoch   4 |  1200/ 2807 batches | lr 4.00 | loss  2.20 | ppl     8.98\n",
            "| epoch   4 |  1300/ 2807 batches | lr 4.00 | loss  2.19 | ppl     8.98\n",
            "| epoch   4 |  1400/ 2807 batches | lr 4.00 | loss  2.19 | ppl     8.90\n",
            "| epoch   4 |  1500/ 2807 batches | lr 4.00 | loss  2.19 | ppl     8.92\n",
            "| epoch   4 |  1600/ 2807 batches | lr 4.00 | loss  2.19 | ppl     8.90\n",
            "| epoch   4 |  1700/ 2807 batches | lr 4.00 | loss  2.18 | ppl     8.85\n",
            "| epoch   4 |  1800/ 2807 batches | lr 4.00 | loss  2.18 | ppl     8.81\n",
            "| epoch   4 |  1900/ 2807 batches | lr 4.00 | loss  2.18 | ppl     8.85\n",
            "| epoch   4 |  2000/ 2807 batches | lr 4.00 | loss  2.17 | ppl     8.74\n",
            "| epoch   4 |  2100/ 2807 batches | lr 4.00 | loss  2.17 | ppl     8.76\n",
            "| epoch   4 |  2200/ 2807 batches | lr 4.00 | loss  2.16 | ppl     8.70\n",
            "| epoch   4 |  2300/ 2807 batches | lr 4.00 | loss  2.17 | ppl     8.77\n",
            "| epoch   4 |  2400/ 2807 batches | lr 4.00 | loss  2.16 | ppl     8.66\n",
            "| epoch   4 |  2500/ 2807 batches | lr 4.00 | loss  2.16 | ppl     8.68\n",
            "| epoch   4 |  2600/ 2807 batches | lr 4.00 | loss  2.16 | ppl     8.64\n",
            "| epoch   4 |  2700/ 2807 batches | lr 4.00 | loss  2.15 | ppl     8.62\n",
            "| epoch   4 |  2800/ 2807 batches | lr 4.00 | loss  2.14 | ppl     8.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | valid loss  1.97 | valid ppl     7.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  . Daomergeecing , of . = Inst 209 Bareccing foomt \n",
            "\n",
            "| epoch   5 |   100/ 2807 batches | lr 4.00 | loss  2.16 | ppl     8.70\n",
            "| epoch   5 |   200/ 2807 batches | lr 4.00 | loss  2.13 | ppl     8.44\n",
            "| epoch   5 |   300/ 2807 batches | lr 4.00 | loss  2.13 | ppl     8.44\n",
            "| epoch   5 |   400/ 2807 batches | lr 4.00 | loss  2.13 | ppl     8.43\n",
            "| epoch   5 |   500/ 2807 batches | lr 4.00 | loss  2.13 | ppl     8.38\n",
            "| epoch   5 |   600/ 2807 batches | lr 4.00 | loss  2.12 | ppl     8.36\n",
            "| epoch   5 |   700/ 2807 batches | lr 4.00 | loss  2.13 | ppl     8.38\n",
            "| epoch   5 |   800/ 2807 batches | lr 4.00 | loss  2.12 | ppl     8.35\n",
            "| epoch   5 |   900/ 2807 batches | lr 4.00 | loss  2.12 | ppl     8.35\n",
            "| epoch   5 |  1000/ 2807 batches | lr 4.00 | loss  2.12 | ppl     8.34\n",
            "| epoch   5 |  1100/ 2807 batches | lr 4.00 | loss  2.11 | ppl     8.26\n",
            "| epoch   5 |  1200/ 2807 batches | lr 4.00 | loss  2.11 | ppl     8.25\n",
            "| epoch   5 |  1300/ 2807 batches | lr 4.00 | loss  2.11 | ppl     8.23\n",
            "| epoch   5 |  1400/ 2807 batches | lr 4.00 | loss  2.10 | ppl     8.18\n",
            "| epoch   5 |  1500/ 2807 batches | lr 4.00 | loss  2.10 | ppl     8.20\n",
            "| epoch   5 |  1600/ 2807 batches | lr 4.00 | loss  2.10 | ppl     8.20\n",
            "| epoch   5 |  1700/ 2807 batches | lr 4.00 | loss  2.10 | ppl     8.16\n",
            "| epoch   5 |  1800/ 2807 batches | lr 4.00 | loss  2.10 | ppl     8.13\n",
            "| epoch   5 |  1900/ 2807 batches | lr 4.00 | loss  2.10 | ppl     8.18\n",
            "| epoch   5 |  2000/ 2807 batches | lr 4.00 | loss  2.09 | ppl     8.09\n",
            "| epoch   5 |  2100/ 2807 batches | lr 4.00 | loss  2.09 | ppl     8.11\n",
            "| epoch   5 |  2200/ 2807 batches | lr 4.00 | loss  2.09 | ppl     8.07\n",
            "| epoch   5 |  2300/ 2807 batches | lr 4.00 | loss  2.10 | ppl     8.14\n",
            "| epoch   5 |  2400/ 2807 batches | lr 4.00 | loss  2.08 | ppl     8.03\n",
            "| epoch   5 |  2500/ 2807 batches | lr 4.00 | loss  2.09 | ppl     8.06\n",
            "| epoch   5 |  2600/ 2807 batches | lr 4.00 | loss  2.09 | ppl     8.05\n",
            "| epoch   5 |  2700/ 2807 batches | lr 4.00 | loss  2.08 | ppl     8.03\n",
            "| epoch   5 |  2800/ 2807 batches | lr 4.00 | loss  2.07 | ppl     7.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | valid loss  1.89 | valid ppl     6.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " ry efferinh loyald three of thele infopca pocg trr \n",
            "\n",
            "CPU times: user 3min 13s, sys: 1min, total: 4min 13s\n",
            "Wall time: 4min 14s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlDPKPgIv2l7",
        "colab_type": "text"
      },
      "source": [
        "Сравнение.   \n",
        "Исходный бейзлайн - | end of epoch 5 | valid loss 1.49 | valid ppl 4.42   \n",
        "Вывод: Чуть хуже бейзлайна"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYcm5oAlwD_i",
        "colab_type": "text"
      },
      "source": [
        "## 5) Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08Ij6IpownzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens = len(dataset.idx2symbol)\n",
        "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3).to(device)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyv4SHXcuNCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ELikaVzxMXb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebc166b5-d0c3-4f7c-f408-e70fb7cf9fdc"
      },
      "source": [
        "%%time\n",
        "\n",
        "with torch.no_grad():\n",
        "    print('sample:\\n', generate(50), '\\n')\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    train(optimizer)\n",
        "    val_loss = evaluate(val_loader)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "        epoch, val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        lr /= 4.0\n",
        "    with torch.no_grad():\n",
        "        print('sample:\\n', generate(50), '\\n')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample:\n",
            " <sos>lṣṃø殻ŁMòhé§ม’รuửხâK2еณ動α×@大t〉Đ̃(ûXÎìm=ửSả/m%0šხلา \n",
            "\n",
            "| epoch   1 |   100/ 2807 batches | lr 0.0010 | loss  3.65 | ppl    38.63\n",
            "| epoch   1 |   200/ 2807 batches | lr 0.0010 | loss  3.13 | ppl    22.98\n",
            "| epoch   1 |   300/ 2807 batches | lr 0.0010 | loss  2.85 | ppl    17.33\n",
            "| epoch   1 |   400/ 2807 batches | lr 0.0010 | loss  2.67 | ppl    14.46\n",
            "| epoch   1 |   500/ 2807 batches | lr 0.0010 | loss  2.52 | ppl    12.46\n",
            "| epoch   1 |   600/ 2807 batches | lr 0.0010 | loss  2.43 | ppl    11.35\n",
            "| epoch   1 |   700/ 2807 batches | lr 0.0010 | loss  2.37 | ppl    10.72\n",
            "| epoch   1 |   800/ 2807 batches | lr 0.0010 | loss  2.32 | ppl    10.22\n",
            "| epoch   1 |   900/ 2807 batches | lr 0.0010 | loss  2.29 | ppl     9.88\n",
            "| epoch   1 |  1000/ 2807 batches | lr 0.0010 | loss  2.26 | ppl     9.57\n",
            "| epoch   1 |  1100/ 2807 batches | lr 0.0010 | loss  2.22 | ppl     9.22\n",
            "| epoch   1 |  1200/ 2807 batches | lr 0.0010 | loss  2.20 | ppl     9.03\n",
            "| epoch   1 |  1300/ 2807 batches | lr 0.0010 | loss  2.18 | ppl     8.83\n",
            "| epoch   1 |  1400/ 2807 batches | lr 0.0010 | loss  2.16 | ppl     8.65\n",
            "| epoch   1 |  1500/ 2807 batches | lr 0.0010 | loss  2.14 | ppl     8.52\n",
            "| epoch   1 |  1600/ 2807 batches | lr 0.0010 | loss  2.13 | ppl     8.41\n",
            "| epoch   1 |  1700/ 2807 batches | lr 0.0010 | loss  2.11 | ppl     8.27\n",
            "| epoch   1 |  1800/ 2807 batches | lr 0.0010 | loss  2.10 | ppl     8.15\n",
            "| epoch   1 |  1900/ 2807 batches | lr 0.0010 | loss  2.09 | ppl     8.08\n",
            "| epoch   1 |  2000/ 2807 batches | lr 0.0010 | loss  2.07 | ppl     7.91\n",
            "| epoch   1 |  2100/ 2807 batches | lr 0.0010 | loss  2.06 | ppl     7.85\n",
            "| epoch   1 |  2200/ 2807 batches | lr 0.0010 | loss  2.05 | ppl     7.74\n",
            "| epoch   1 |  2300/ 2807 batches | lr 0.0010 | loss  2.05 | ppl     7.73\n",
            "| epoch   1 |  2400/ 2807 batches | lr 0.0010 | loss  2.02 | ppl     7.56\n",
            "| epoch   1 |  2500/ 2807 batches | lr 0.0010 | loss  2.01 | ppl     7.50\n",
            "| epoch   1 |  2600/ 2807 batches | lr 0.0010 | loss  2.01 | ppl     7.46\n",
            "| epoch   1 |  2700/ 2807 batches | lr 0.0010 | loss  2.00 | ppl     7.37\n",
            "| epoch   1 |  2800/ 2807 batches | lr 0.0010 | loss  1.98 | ppl     7.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | valid loss  1.78 | valid ppl     5.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " ates was polled by Wegembth compencision sederalta \n",
            "\n",
            "| epoch   2 |   100/ 2807 batches | lr 0.0010 | loss  1.99 | ppl     7.33\n",
            "| epoch   2 |   200/ 2807 batches | lr 0.0010 | loss  1.96 | ppl     7.09\n",
            "| epoch   2 |   300/ 2807 batches | lr 0.0010 | loss  1.95 | ppl     7.03\n",
            "| epoch   2 |   400/ 2807 batches | lr 0.0010 | loss  1.95 | ppl     7.00\n",
            "| epoch   2 |   500/ 2807 batches | lr 0.0010 | loss  1.94 | ppl     6.95\n",
            "| epoch   2 |   600/ 2807 batches | lr 0.0010 | loss  1.93 | ppl     6.89\n",
            "| epoch   2 |   700/ 2807 batches | lr 0.0010 | loss  1.93 | ppl     6.86\n",
            "| epoch   2 |   800/ 2807 batches | lr 0.0010 | loss  1.92 | ppl     6.81\n",
            "| epoch   2 |   900/ 2807 batches | lr 0.0010 | loss  1.91 | ppl     6.78\n",
            "| epoch   2 |  1000/ 2807 batches | lr 0.0010 | loss  1.91 | ppl     6.78\n",
            "| epoch   2 |  1100/ 2807 batches | lr 0.0010 | loss  1.90 | ppl     6.67\n",
            "| epoch   2 |  1200/ 2807 batches | lr 0.0010 | loss  1.90 | ppl     6.67\n",
            "| epoch   2 |  1300/ 2807 batches | lr 0.0010 | loss  1.89 | ppl     6.63\n",
            "| epoch   2 |  1400/ 2807 batches | lr 0.0010 | loss  1.88 | ppl     6.53\n",
            "| epoch   2 |  1500/ 2807 batches | lr 0.0010 | loss  1.88 | ppl     6.53\n",
            "| epoch   2 |  1600/ 2807 batches | lr 0.0010 | loss  1.88 | ppl     6.53\n",
            "| epoch   2 |  1700/ 2807 batches | lr 0.0010 | loss  1.87 | ppl     6.48\n",
            "| epoch   2 |  1800/ 2807 batches | lr 0.0010 | loss  1.87 | ppl     6.46\n",
            "| epoch   2 |  1900/ 2807 batches | lr 0.0010 | loss  1.87 | ppl     6.49\n",
            "| epoch   2 |  2000/ 2807 batches | lr 0.0010 | loss  1.86 | ppl     6.41\n",
            "| epoch   2 |  2100/ 2807 batches | lr 0.0010 | loss  1.86 | ppl     6.43\n",
            "| epoch   2 |  2200/ 2807 batches | lr 0.0010 | loss  1.85 | ppl     6.38\n",
            "| epoch   2 |  2300/ 2807 batches | lr 0.0010 | loss  1.86 | ppl     6.40\n",
            "| epoch   2 |  2400/ 2807 batches | lr 0.0010 | loss  1.84 | ppl     6.30\n",
            "| epoch   2 |  2500/ 2807 batches | lr 0.0010 | loss  1.84 | ppl     6.29\n",
            "| epoch   2 |  2600/ 2807 batches | lr 0.0010 | loss  1.84 | ppl     6.32\n",
            "| epoch   2 |  2700/ 2807 batches | lr 0.0010 | loss  1.84 | ppl     6.29\n",
            "| epoch   2 |  2800/ 2807 batches | lr 0.0010 | loss  1.82 | ppl     6.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | valid loss  1.61 | valid ppl     5.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " th Gevern 's deleaity \" ( 530 , <unk> . Holding wa \n",
            "\n",
            "| epoch   3 |   100/ 2807 batches | lr 0.0010 | loss  1.84 | ppl     6.32\n",
            "| epoch   3 |   200/ 2807 batches | lr 0.0010 | loss  1.82 | ppl     6.17\n",
            "| epoch   3 |   300/ 2807 batches | lr 0.0010 | loss  1.82 | ppl     6.17\n",
            "| epoch   3 |   400/ 2807 batches | lr 0.0010 | loss  1.82 | ppl     6.15\n",
            "| epoch   3 |   500/ 2807 batches | lr 0.0010 | loss  1.81 | ppl     6.13\n",
            "| epoch   3 |   600/ 2807 batches | lr 0.0010 | loss  1.81 | ppl     6.10\n",
            "| epoch   3 |   700/ 2807 batches | lr 0.0010 | loss  1.81 | ppl     6.10\n",
            "| epoch   3 |   800/ 2807 batches | lr 0.0010 | loss  1.80 | ppl     6.07\n",
            "| epoch   3 |   900/ 2807 batches | lr 0.0010 | loss  1.80 | ppl     6.07\n",
            "| epoch   3 |  1000/ 2807 batches | lr 0.0010 | loss  1.80 | ppl     6.08\n",
            "| epoch   3 |  1100/ 2807 batches | lr 0.0010 | loss  1.79 | ppl     6.01\n",
            "| epoch   3 |  1200/ 2807 batches | lr 0.0010 | loss  1.80 | ppl     6.04\n",
            "| epoch   3 |  1300/ 2807 batches | lr 0.0010 | loss  1.79 | ppl     6.01\n",
            "| epoch   3 |  1400/ 2807 batches | lr 0.0010 | loss  1.78 | ppl     5.93\n",
            "| epoch   3 |  1500/ 2807 batches | lr 0.0010 | loss  1.78 | ppl     5.95\n",
            "| epoch   3 |  1600/ 2807 batches | lr 0.0010 | loss  1.79 | ppl     5.97\n",
            "| epoch   3 |  1700/ 2807 batches | lr 0.0010 | loss  1.78 | ppl     5.93\n",
            "| epoch   3 |  1800/ 2807 batches | lr 0.0010 | loss  1.78 | ppl     5.93\n",
            "| epoch   3 |  1900/ 2807 batches | lr 0.0010 | loss  1.79 | ppl     5.99\n",
            "| epoch   3 |  2000/ 2807 batches | lr 0.0010 | loss  1.78 | ppl     5.91\n",
            "| epoch   3 |  2100/ 2807 batches | lr 0.0010 | loss  1.78 | ppl     5.94\n",
            "| epoch   3 |  2200/ 2807 batches | lr 0.0010 | loss  1.78 | ppl     5.90\n",
            "| epoch   3 |  2300/ 2807 batches | lr 0.0010 | loss  1.78 | ppl     5.93\n",
            "| epoch   3 |  2400/ 2807 batches | lr 0.0010 | loss  1.77 | ppl     5.86\n",
            "| epoch   3 |  2500/ 2807 batches | lr 0.0010 | loss  1.77 | ppl     5.85\n",
            "| epoch   3 |  2600/ 2807 batches | lr 0.0010 | loss  1.77 | ppl     5.90\n",
            "| epoch   3 |  2700/ 2807 batches | lr 0.0010 | loss  1.77 | ppl     5.87\n",
            "| epoch   3 |  2800/ 2807 batches | lr 0.0010 | loss  1.76 | ppl     5.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | valid loss  1.54 | valid ppl     4.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  sick made cefter cCountryme . To <unk> , whe affe \n",
            "\n",
            "| epoch   4 |   100/ 2807 batches | lr 0.0010 | loss  1.78 | ppl     5.93\n",
            "| epoch   4 |   200/ 2807 batches | lr 0.0010 | loss  1.75 | ppl     5.78\n",
            "| epoch   4 |   300/ 2807 batches | lr 0.0010 | loss  1.76 | ppl     5.81\n",
            "| epoch   4 |   400/ 2807 batches | lr 0.0010 | loss  1.76 | ppl     5.79\n",
            "| epoch   4 |   500/ 2807 batches | lr 0.0010 | loss  1.75 | ppl     5.78\n",
            "| epoch   4 |   600/ 2807 batches | lr 0.0010 | loss  1.75 | ppl     5.75\n",
            "| epoch   4 |   700/ 2807 batches | lr 0.0010 | loss  1.75 | ppl     5.77\n",
            "| epoch   4 |   800/ 2807 batches | lr 0.0010 | loss  1.75 | ppl     5.74\n",
            "| epoch   4 |   900/ 2807 batches | lr 0.0010 | loss  1.75 | ppl     5.74\n",
            "| epoch   4 |  1000/ 2807 batches | lr 0.0010 | loss  1.75 | ppl     5.76\n",
            "| epoch   4 |  1100/ 2807 batches | lr 0.0010 | loss  1.74 | ppl     5.71\n",
            "| epoch   4 |  1200/ 2807 batches | lr 0.0010 | loss  1.75 | ppl     5.75\n",
            "| epoch   4 |  1300/ 2807 batches | lr 0.0010 | loss  1.74 | ppl     5.72\n",
            "| epoch   4 |  1400/ 2807 batches | lr 0.0010 | loss  1.73 | ppl     5.63\n",
            "| epoch   4 |  1500/ 2807 batches | lr 0.0010 | loss  1.73 | ppl     5.67\n",
            "| epoch   4 |  1600/ 2807 batches | lr 0.0010 | loss  1.74 | ppl     5.68\n",
            "| epoch   4 |  1700/ 2807 batches | lr 0.0010 | loss  1.73 | ppl     5.66\n",
            "| epoch   4 |  1800/ 2807 batches | lr 0.0010 | loss  1.73 | ppl     5.66\n",
            "| epoch   4 |  1900/ 2807 batches | lr 0.0010 | loss  1.74 | ppl     5.72\n",
            "| epoch   4 |  2000/ 2807 batches | lr 0.0010 | loss  1.73 | ppl     5.66\n",
            "| epoch   4 |  2100/ 2807 batches | lr 0.0010 | loss  1.74 | ppl     5.70\n",
            "| epoch   4 |  2200/ 2807 batches | lr 0.0010 | loss  1.74 | ppl     5.67\n",
            "| epoch   4 |  2300/ 2807 batches | lr 0.0010 | loss  1.74 | ppl     5.69\n",
            "| epoch   4 |  2400/ 2807 batches | lr 0.0010 | loss  1.73 | ppl     5.61\n",
            "| epoch   4 |  2500/ 2807 batches | lr 0.0010 | loss  1.73 | ppl     5.61\n",
            "| epoch   4 |  2600/ 2807 batches | lr 0.0010 | loss  1.73 | ppl     5.67\n",
            "| epoch   4 |  2700/ 2807 batches | lr 0.0010 | loss  1.73 | ppl     5.66\n",
            "| epoch   4 |  2800/ 2807 batches | lr 0.0010 | loss  1.72 | ppl     5.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | valid loss  1.50 | valid ppl     4.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            " swar ameas perfilman more , breaken recestier , <u \n",
            "\n",
            "| epoch   5 |   100/ 2807 batches | lr 0.0010 | loss  1.74 | ppl     5.70\n",
            "| epoch   5 |   200/ 2807 batches | lr 0.0010 | loss  1.72 | ppl     5.58\n",
            "| epoch   5 |   300/ 2807 batches | lr 0.0010 | loss  1.72 | ppl     5.60\n",
            "| epoch   5 |   400/ 2807 batches | lr 0.0010 | loss  1.72 | ppl     5.60\n",
            "| epoch   5 |   500/ 2807 batches | lr 0.0010 | loss  1.72 | ppl     5.58\n",
            "| epoch   5 |   600/ 2807 batches | lr 0.0010 | loss  1.72 | ppl     5.56\n",
            "| epoch   5 |   700/ 2807 batches | lr 0.0010 | loss  1.72 | ppl     5.58\n",
            "| epoch   5 |   800/ 2807 batches | lr 0.0010 | loss  1.71 | ppl     5.55\n",
            "| epoch   5 |   900/ 2807 batches | lr 0.0010 | loss  1.72 | ppl     5.56\n",
            "| epoch   5 |  1000/ 2807 batches | lr 0.0010 | loss  1.72 | ppl     5.57\n",
            "| epoch   5 |  1100/ 2807 batches | lr 0.0010 | loss  1.71 | ppl     5.52\n",
            "| epoch   5 |  1200/ 2807 batches | lr 0.0010 | loss  1.72 | ppl     5.57\n",
            "| epoch   5 |  1300/ 2807 batches | lr 0.0010 | loss  1.71 | ppl     5.53\n",
            "| epoch   5 |  1400/ 2807 batches | lr 0.0010 | loss  1.70 | ppl     5.48\n",
            "| epoch   5 |  1500/ 2807 batches | lr 0.0010 | loss  1.70 | ppl     5.50\n",
            "| epoch   5 |  1600/ 2807 batches | lr 0.0010 | loss  1.71 | ppl     5.52\n",
            "| epoch   5 |  1700/ 2807 batches | lr 0.0010 | loss  1.70 | ppl     5.49\n",
            "| epoch   5 |  1800/ 2807 batches | lr 0.0010 | loss  1.70 | ppl     5.49\n",
            "| epoch   5 |  1900/ 2807 batches | lr 0.0010 | loss  1.72 | ppl     5.56\n",
            "| epoch   5 |  2000/ 2807 batches | lr 0.0010 | loss  1.71 | ppl     5.50\n",
            "| epoch   5 |  2100/ 2807 batches | lr 0.0010 | loss  1.71 | ppl     5.54\n",
            "| epoch   5 |  2200/ 2807 batches | lr 0.0010 | loss  1.71 | ppl     5.52\n",
            "| epoch   5 |  2300/ 2807 batches | lr 0.0010 | loss  1.71 | ppl     5.54\n",
            "| epoch   5 |  2400/ 2807 batches | lr 0.0010 | loss  1.70 | ppl     5.46\n",
            "| epoch   5 |  2500/ 2807 batches | lr 0.0010 | loss  1.70 | ppl     5.47\n",
            "| epoch   5 |  2600/ 2807 batches | lr 0.0010 | loss  1.71 | ppl     5.52\n",
            "| epoch   5 |  2700/ 2807 batches | lr 0.0010 | loss  1.71 | ppl     5.51\n",
            "| epoch   5 |  2800/ 2807 batches | lr 0.0010 | loss  1.70 | ppl     5.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | valid loss  1.48 | valid ppl     4.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "sample:\n",
            "  I , a nersland and the national maileat day : amo \n",
            "\n",
            "CPU times: user 3min 11s, sys: 58.8 s, total: 4min 10s\n",
            "Wall time: 4min 11s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JvfbWil1pLi",
        "colab_type": "text"
      },
      "source": [
        "Сравнение.   \n",
        "Исходный бейзлайн - | end of epoch 5 | valid loss 1.49 | valid ppl 4.42   \n",
        "Вывод: Чуть-чуть лучше бейзлайна"
      ]
    }
  ]
}