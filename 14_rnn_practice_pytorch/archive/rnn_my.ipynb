{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "from wiki2_utils import WikiText2Dataset\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100\n",
    "\n",
    "eval_batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### загрузчики датасетов (с использованием нового класса)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = WikiText2Dataset(batch_size=batch_size, eval_batch_size=eval_batch_size, \n",
    "                           sequence_length=sequence_length)\n",
    "\n",
    "train_loader = dataset.get_train_loader()\n",
    "val_loader = dataset.get_val_loader()\n",
    "test_loader = dataset.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))  # 30x128x128(ninp - размерность единицы на входе)\n",
    "        output, hidden = self.rnn(emb, hidden)  # 30x128x128(nhid - размерность единицы на выходе (внутр. слоя))\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(dataset.idx2symbol)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, (data, targets) in enumerate(data_loader):\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = len(dataset.idx2symbol)\n",
    "    for batch, (data, targets) in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_loader) // sequence_length, lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(dataset.idx2symbol)\n",
    "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = dataset.idx2symbol[s_idx]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " ヴPล?≤'·μëプ大l7к攻μตả์HìαÎễ戦्?ย動ăuIW>½リ‑Ø⁄ôósァ隊e½dúū空 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/ 2807 batches | lr 4.00 | loss  3.59 | ppl    36.36\n",
      "| epoch   1 |   200/ 2807 batches | lr 4.00 | loss  3.29 | ppl    26.72\n",
      "| epoch   1 |   300/ 2807 batches | lr 4.00 | loss  3.25 | ppl    25.80\n",
      "| epoch   1 |   400/ 2807 batches | lr 4.00 | loss  3.22 | ppl    25.12\n",
      "| epoch   1 |   500/ 2807 batches | lr 4.00 | loss  3.18 | ppl    24.11\n",
      "| epoch   1 |   600/ 2807 batches | lr 4.00 | loss  3.06 | ppl    21.39\n",
      "| epoch   1 |   700/ 2807 batches | lr 4.00 | loss  2.98 | ppl    19.70\n",
      "| epoch   1 |   800/ 2807 batches | lr 4.00 | loss  2.90 | ppl    18.20\n",
      "| epoch   1 |   900/ 2807 batches | lr 4.00 | loss  2.82 | ppl    16.83\n",
      "| epoch   1 |  1000/ 2807 batches | lr 4.00 | loss  2.74 | ppl    15.48\n",
      "| epoch   1 |  1100/ 2807 batches | lr 4.00 | loss  2.63 | ppl    13.90\n",
      "| epoch   1 |  1200/ 2807 batches | lr 4.00 | loss  2.56 | ppl    12.95\n",
      "| epoch   1 |  1300/ 2807 batches | lr 4.00 | loss  2.50 | ppl    12.24\n",
      "| epoch   1 |  1400/ 2807 batches | lr 4.00 | loss  2.45 | ppl    11.64\n",
      "| epoch   1 |  1500/ 2807 batches | lr 4.00 | loss  2.42 | ppl    11.28\n",
      "| epoch   1 |  1600/ 2807 batches | lr 4.00 | loss  2.39 | ppl    10.89\n",
      "| epoch   1 |  1700/ 2807 batches | lr 4.00 | loss  2.35 | ppl    10.51\n",
      "| epoch   1 |  1800/ 2807 batches | lr 4.00 | loss  2.32 | ppl    10.18\n",
      "| epoch   1 |  1900/ 2807 batches | lr 4.00 | loss  2.30 | ppl     9.94\n",
      "| epoch   1 |  2000/ 2807 batches | lr 4.00 | loss  2.27 | ppl     9.65\n",
      "| epoch   1 |  2100/ 2807 batches | lr 4.00 | loss  2.25 | ppl     9.45\n",
      "| epoch   1 |  2200/ 2807 batches | lr 4.00 | loss  2.22 | ppl     9.22\n",
      "| epoch   1 |  2300/ 2807 batches | lr 4.00 | loss  2.21 | ppl     9.16\n",
      "| epoch   1 |  2400/ 2807 batches | lr 4.00 | loss  2.19 | ppl     8.90\n",
      "| epoch   1 |  2500/ 2807 batches | lr 4.00 | loss  2.17 | ppl     8.79\n",
      "| epoch   1 |  2600/ 2807 batches | lr 4.00 | loss  2.16 | ppl     8.64\n",
      "| epoch   1 |  2700/ 2807 batches | lr 4.00 | loss  2.14 | ppl     8.51\n",
      "| epoch   1 |  2800/ 2807 batches | lr 4.00 | loss  2.11 | ppl     8.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.94 | valid ppl     6.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  efosoll . Mach sabs , the ringitt wheir in funtuo \n",
      "\n",
      "| epoch   2 |   100/ 2807 batches | lr 4.00 | loss  2.12 | ppl     8.37\n",
      "| epoch   2 |   200/ 2807 batches | lr 4.00 | loss  2.08 | ppl     8.03\n",
      "| epoch   2 |   300/ 2807 batches | lr 4.00 | loss  2.07 | ppl     7.96\n",
      "| epoch   2 |   400/ 2807 batches | lr 4.00 | loss  2.06 | ppl     7.86\n",
      "| epoch   2 |   500/ 2807 batches | lr 4.00 | loss  2.05 | ppl     7.77\n",
      "| epoch   2 |   600/ 2807 batches | lr 4.00 | loss  2.04 | ppl     7.69\n",
      "| epoch   2 |   700/ 2807 batches | lr 4.00 | loss  2.03 | ppl     7.62\n",
      "| epoch   2 |   800/ 2807 batches | lr 4.00 | loss  2.02 | ppl     7.53\n",
      "| epoch   2 |   900/ 2807 batches | lr 4.00 | loss  2.01 | ppl     7.48\n",
      "| epoch   2 |  1000/ 2807 batches | lr 4.00 | loss  2.01 | ppl     7.44\n",
      "| epoch   2 |  1100/ 2807 batches | lr 4.00 | loss  1.99 | ppl     7.30\n",
      "| epoch   2 |  1200/ 2807 batches | lr 4.00 | loss  1.98 | ppl     7.27\n",
      "| epoch   2 |  1300/ 2807 batches | lr 4.00 | loss  1.97 | ppl     7.20\n",
      "| epoch   2 |  1400/ 2807 batches | lr 4.00 | loss  1.96 | ppl     7.08\n",
      "| epoch   2 |  1500/ 2807 batches | lr 4.00 | loss  1.96 | ppl     7.08\n",
      "| epoch   2 |  1600/ 2807 batches | lr 4.00 | loss  1.95 | ppl     7.04\n",
      "| epoch   2 |  1700/ 2807 batches | lr 4.00 | loss  1.94 | ppl     6.97\n",
      "| epoch   2 |  1800/ 2807 batches | lr 4.00 | loss  1.93 | ppl     6.92\n",
      "| epoch   2 |  1900/ 2807 batches | lr 4.00 | loss  1.94 | ppl     6.94\n",
      "| epoch   2 |  2000/ 2807 batches | lr 4.00 | loss  1.92 | ppl     6.83\n",
      "| epoch   2 |  2100/ 2807 batches | lr 4.00 | loss  1.92 | ppl     6.84\n",
      "| epoch   2 |  2200/ 2807 batches | lr 4.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   2 |  2300/ 2807 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch   2 |  2400/ 2807 batches | lr 4.00 | loss  1.90 | ppl     6.67\n",
      "| epoch   2 |  2500/ 2807 batches | lr 4.00 | loss  1.90 | ppl     6.67\n",
      "| epoch   2 |  2600/ 2807 batches | lr 4.00 | loss  1.90 | ppl     6.67\n",
      "| epoch   2 |  2700/ 2807 batches | lr 4.00 | loss  1.89 | ppl     6.64\n",
      "| epoch   2 |  2800/ 2807 batches | lr 4.00 | loss  1.88 | ppl     6.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.67 | valid ppl     5.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  . In 1910 . Tichaid becreting poblost of Namkeed  \n",
      "\n",
      "| epoch   3 |   100/ 2807 batches | lr 4.00 | loss  1.89 | ppl     6.64\n",
      "| epoch   3 |   200/ 2807 batches | lr 4.00 | loss  1.87 | ppl     6.46\n",
      "| epoch   3 |   300/ 2807 batches | lr 4.00 | loss  1.86 | ppl     6.45\n",
      "| epoch   3 |   400/ 2807 batches | lr 4.00 | loss  1.86 | ppl     6.43\n",
      "| epoch   3 |   500/ 2807 batches | lr 4.00 | loss  1.86 | ppl     6.40\n",
      "| epoch   3 |   600/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.35\n",
      "| epoch   3 |   700/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.36\n",
      "| epoch   3 |   800/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.32\n",
      "| epoch   3 |   900/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.32\n",
      "| epoch   3 |  1000/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.32\n",
      "| epoch   3 |  1100/ 2807 batches | lr 4.00 | loss  1.83 | ppl     6.24\n",
      "| epoch   3 |  1200/ 2807 batches | lr 4.00 | loss  1.84 | ppl     6.27\n",
      "| epoch   3 |  1300/ 2807 batches | lr 4.00 | loss  1.83 | ppl     6.22\n",
      "| epoch   3 |  1400/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.14\n",
      "| epoch   3 |  1500/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.17\n",
      "| epoch   3 |  1600/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.18\n",
      "| epoch   3 |  1700/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.13\n",
      "| epoch   3 |  1800/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.11\n",
      "| epoch   3 |  1900/ 2807 batches | lr 4.00 | loss  1.82 | ppl     6.16\n",
      "| epoch   3 |  2000/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.09\n",
      "| epoch   3 |  2100/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.12\n",
      "| epoch   3 |  2200/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.09\n",
      "| epoch   3 |  2300/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.11\n",
      "| epoch   3 |  2400/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.02\n",
      "| epoch   3 |  2500/ 2807 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
      "| epoch   3 |  2600/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.06\n",
      "| epoch   3 |  2700/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.04\n",
      "| epoch   3 |  2800/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.57 | valid ppl     4.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ch where Ceneur 4 land moverse . The Bark drisms . \n",
      "\n",
      "| epoch   4 |   100/ 2807 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
      "| epoch   4 |   200/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.91\n",
      "| epoch   4 |   300/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
      "| epoch   4 |   400/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.93\n",
      "| epoch   4 |   500/ 2807 batches | lr 4.00 | loss  1.78 | ppl     5.91\n",
      "| epoch   4 |   600/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.88\n",
      "| epoch   4 |   700/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "| epoch   4 |   800/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
      "| epoch   4 |   900/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.88\n",
      "| epoch   4 |  1000/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
      "| epoch   4 |  1100/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.83\n",
      "| epoch   4 |  1200/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
      "| epoch   4 |  1300/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.82\n",
      "| epoch   4 |  1400/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.75\n",
      "| epoch   4 |  1500/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.77\n",
      "| epoch   4 |  1600/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.80\n",
      "| epoch   4 |  1700/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "| epoch   4 |  1800/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.75\n",
      "| epoch   4 |  1900/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.82\n",
      "| epoch   4 |  2000/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
      "| epoch   4 |  2100/ 2807 batches | lr 4.00 | loss  1.76 | ppl     5.79\n",
      "| epoch   4 |  2200/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "| epoch   4 |  2300/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
      "| epoch   4 |  2400/ 2807 batches | lr 4.00 | loss  1.74 | ppl     5.71\n",
      "| epoch   4 |  2500/ 2807 batches | lr 4.00 | loss  1.74 | ppl     5.71\n",
      "| epoch   4 |  2600/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "| epoch   4 |  2700/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
      "| epoch   4 |  2800/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.52 | valid ppl     4.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  gatan wroll at Aman Vantin direcparty Kripa canti \n",
      "\n",
      "| epoch   5 |   100/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
      "| epoch   5 |   200/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   5 |   300/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.67\n",
      "| epoch   5 |   400/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.67\n",
      "| epoch   5 |   500/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "| epoch   5 |   600/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "| epoch   5 |   700/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.65\n",
      "| epoch   5 |   800/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   5 |   900/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "| epoch   5 |  1000/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "| epoch   5 |  1100/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   5 |  1200/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   5 |  1300/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   5 |  1400/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   5 |  1500/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   5 |  1600/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "| epoch   5 |  1700/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   5 |  1800/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   5 |  1900/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   5 |  2000/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   5 |  2100/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   5 |  2200/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   5 |  2300/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   5 |  2400/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.51\n",
      "| epoch   5 |  2500/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.52\n",
      "| epoch   5 |  2600/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   5 |  2700/ 2807 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   5 |  2800/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.49 | valid ppl     4.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ile east by the serior solds depill is scribited p \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t1 = generate(10000, 1.)\n",
    "t15 = generate(10000, 1.5)\n",
    "t075 = generate(10000, 0.75)\n",
    "with open('./generated075.txt', 'w', encoding=\"utf-8\") as outf:\n",
    "    outf.write(t075)\n",
    "with open('./generated1.txt', 'w', encoding=\"utf-8\") as outf:\n",
    "    outf.write(t1)\n",
    "with open('./generated15.txt', 'w', encoding=\"utf-8\") as outf:\n",
    "    outf.write(t15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Мой разбор generated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        '''\n",
    "        генерируется случайный символ согласно вероятностям, \n",
    "        соответствующим каждому из этих символов. Причем на входе s_weights, \n",
    "        как я понял, может и не быть вероятностями (то есть не обязательно  \n",
    "        sum(s_weights) = 1, как в нашем случае). В этом случае просто нормируется на 1.\n",
    "        '''\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = dataset.idx2symbol[s_idx]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[268]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1, 1).mul(ntokens).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 67,  30, 111],\n",
       "        [ 47, 207, 143]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 3).mul(ntokens).long()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = None\n",
    "out = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden = model(x, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 286])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 128])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 128])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 286])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_weights = output.squeeze().data.div(1.).exp()\n",
    "s_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7],\n",
       "        [ 4],\n",
       "        [48]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_idx = torch.multinomial(s_weights, 1)\n",
    "s_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_weights_test = torch.tensor([0.2, 0.3, 0.5])\n",
    "torch.multinomial(s_weights_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 2, 2, 2, 1, 2, 1, 2, 0])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_weights_test = torch.tensor([2., 3., 5.])\n",
    "torch.multinomial(s_weights_test, 10, replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.3000, 0.5000])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_weights_test = torch.tensor([0.2, 0.3, 0.5])\n",
    "s_weights_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
